{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7593881-3ebd-4071-80ee-8648eedf5d23",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a56c0a-2176-40ff-90e3-eb3afc31c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "# import cv2  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3646e2-1daa-4b0d-b200-894fae0bc3e6",
   "metadata": {},
   "source": [
    "# AffectNet Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d59eaf-cc18-42ad-bda1-0cfd706d4192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded !\n"
     ]
    }
   ],
   "source": [
    "class AffectNetHqDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        # 'dataset' is now a subset of the original dataset\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = load_dataset(\"Piro17/affectnethq\", split='train')\n",
    "\n",
    "# Split the dataset into train and test subsets\n",
    "train_size = int(0.01 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_subset, test_subset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation((-10, 10)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create the dataset and dataloader using the subsets\n",
    "train_dataset = AffectNetHqDataset(Subset(full_dataset, train_subset.indices), transform=train_transform)\n",
    "test_dataset = AffectNetHqDataset(Subset(full_dataset, test_subset.indices), transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"Dataset Loaded !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\MCE30\\\\Desktop\\\\SAR\\\\M2 SAR\\\\MLA\\\\Projet\\\\RAF-DB\\\\Image\\\\aligned/list_partition_label.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m test_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     transforms\u001b[39m.\u001b[39mResize((\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)),\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m ])\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Assuming you have already defined full_dataset, train_subset, and test_subset\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m RAFDBDataset(root_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:/Users/MCE30/Desktop/SAR/M2 SAR/MLA/Projet/RAF-DB/Image/aligned/train\u001b[39;49m\u001b[39m\"\u001b[39;49m, label_dir \u001b[39m=\u001b[39;49m \u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mMCE30\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDesktop\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mSAR\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mM2 SAR\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mMLA\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mProjet\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mRAF-DB\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mImage\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39maligned\u001b[39;49m\u001b[39m'\u001b[39;49m, transform\u001b[39m=\u001b[39;49mtrain_transform)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m test_dataset \u001b[39m=\u001b[39m RAFDBDataset(root_dir\u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mMCE30\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDesktop\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSAR\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mM2 SAR\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mMLA\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mProjet\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mRAF-DB\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mImage\u001b[39m\u001b[39m\\\u001b[39m\u001b[39maligned\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, label_dir \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mMCE30\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDesktop\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSAR\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mM2 SAR\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mMLA\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mProjet\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mRAF-DB\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mImage\u001b[39m\u001b[39m\\\u001b[39m\u001b[39maligned\u001b[39m\u001b[39m'\u001b[39m,transform\u001b[39m=\u001b[39mtest_transform)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_dir \u001b[39m=\u001b[39m label_dir\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m transform\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_paths \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_data()\n",
      "\u001b[1;32m/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m image_paths \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m labels_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_dir, \u001b[39m'\u001b[39m\u001b[39mlist_partition_label.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(labels_file_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     lines \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    302\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\MCE30\\\\Desktop\\\\SAR\\\\M2 SAR\\\\MLA\\\\Projet\\\\RAF-DB\\\\Image\\\\aligned/list_partition_label.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class RAFDBDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.labels, self.image_paths = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        labels = []\n",
    "        image_paths = []\n",
    "\n",
    "        labels_file_path = os.path.join(self.label_dir, 'list_partition_label.txt')\n",
    "        with open(labels_file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(' ')\n",
    "                label = int(parts[1])\n",
    "                image_path = os.path.join(self.root_dir, 'aligned', parts[0])\n",
    "                labels.append(label)\n",
    "                image_paths.append(image_path)\n",
    "\n",
    "        return labels, image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "\n",
    "# Transform function for training data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=(-10, 10)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Transform function for test data\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Assuming you have already defined full_dataset, train_subset, and test_subset\n",
    "train_dataset = RAFDBDataset(root_dir=\"C:/Users/MCE30/Desktop/SAR/M2 SAR/MLA/Projet/RAF-DB/Image/aligned/train\", label_dir = r'C:\\Users\\MCE30\\Desktop\\SAR\\M2 SAR\\MLA\\Projet\\RAF-DB\\Image\\aligned', transform=train_transform)\n",
    "test_dataset = RAFDBDataset(root_dir=r'C:\\Users\\MCE30\\Desktop\\SAR\\M2 SAR\\MLA\\Projet\\RAF-DB\\Image\\aligned\\test', label_dir = r'C:\\Users\\MCE30\\Desktop\\SAR\\M2 SAR\\MLA\\Projet\\RAF-DB\\Image\\aligned',transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"RAF-DB Dataset Loaded !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da01978-38c3-4d4d-b44f-bccf450b0107",
   "metadata": {},
   "source": [
    "# PAL Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a28e3848-bc40-453c-a933-7924c65cec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrivilegedAttributionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrivilegedAttributionLoss, self).__init__()\n",
    "\n",
    "    def forward(self, attribution_maps, prior_maps):\n",
    "        \"\"\"\n",
    "        Compute the Privileged Attribution Loss (PAL).\n",
    "\n",
    "        Args:\n",
    "            attribution_maps (torch.Tensor): Attribution maps (a_l) from your model.\n",
    "            prior_maps (torch.Tensor): Prior maps (a*) that highlight certain regions.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: PAL loss value.\n",
    "        \"\"\"\n",
    "        # Calculate mean and standard deviation of attribution maps (a_l)\n",
    "        mean_al = torch.mean(attribution_maps)\n",
    "        std_al = torch.std(attribution_maps)\n",
    "\n",
    "        # Calculate the PAL loss as described in the provided text\n",
    "        pal_loss = -torch.sum((attribution_maps - mean_al) / std_al * prior_maps)\n",
    "\n",
    "        return pal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd2710-3fc1-424b-a31f-a26ad66e0411",
   "metadata": {},
   "source": [
    "# Modèle VGGFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718520e0-403d-4452-b039-1a54cecf2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGFace(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFace, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Premier bloc\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Deuxième bloc\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Troisième bloc\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Quatrième bloc\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Cinquième bloc\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # self.last_conv_layer = self.conv_layers[-1]\n",
    "        # self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.fc_layers = nn.Sequential(\n",
    "        #     nn.Linear(512, 1024),  \n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Dropout(0.5),\n",
    "        #     nn.Linear(1024, 7),\n",
    "        #     nn.Softmax(dim=1)\n",
    "        # )\n",
    "\n",
    "        self.last_conv_layer = self.conv_layers[-1]\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512, 4096),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 7),  # Assuming there are 7 classes in the RAF-DB dataset\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Attribute to store the gradients of the last convolutional layer\n",
    "        self.last_conv_gradients = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    def capture_last_conv_gradients(self):\n",
    "        return self.last_conv_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef5ae4b-1dc5-479b-ac82-a23740fad6b9",
   "metadata": {},
   "source": [
    "# Création des modèles pour les boucles d'entraînements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02cc3c04-5c13-4cdc-90cd-644b0ac1af35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle pré-entraîné VGG16\n",
    "# base_model = torchvision.models.vgg16(pretrained=True)\n",
    "# Supprimer la dernière couche entièrement connectée\n",
    "# base_model.classifier = nn.Sequential(*list(base_model.classifier.children())[:-1])\n",
    "base_model = VGGFace()\n",
    "# Ajouter une nouvelle couche adaptée à 7 classes\n",
    "num_classes = 7\n",
    "classifier_layer = nn.Linear(4096, num_classes)\n",
    "model = nn.Sequential(base_model, classifier_layer)\n",
    "\n",
    "# Afficher la structure du modèle\n",
    "# summary(model, (3, 224, 224))  # Assurez-vous d'ajuster les dimensions en fonction de vos données\n",
    "\n",
    "\n",
    "# Identifier la dernière couche de convolution\n",
    "last_conv_layer = base_model.last_conv_layer\n",
    "# last_conv_layer = model[0].features[28]\n",
    "print(last_conv_layer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=4e-5)\n",
    "\n",
    "# Fonction pour enregistrer le gradient\n",
    "def save_gradient(grad):\n",
    "    global conv_output_gradient\n",
    "    conv_output_gradient = grad\n",
    "\n",
    "# Attacher un hook pour enregistrer le gradient\n",
    "last_conv_layer.register_backward_hook(lambda module, grad_in, grad_out: save_gradient(grad_out[0]))\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd003fe-c3ee-4645-8e50-bded71fbfe03",
   "metadata": {},
   "source": [
    "# Boucle d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f23b2323-e299-4fd9-812d-34ad57321d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# outputs = model(images)    # Pour VGG16\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate the gradient of the output with respect to the input\u001b[39;00m\n\u001b[1;32m     17\u001b[0m grad_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(outputs\u001b[38;5;241m.\u001b[39msize(), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Set requires_grad=True\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m input_grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Calculate the gradient times the input\u001b[39;00m\n\u001b[1;32m     21\u001b[0m grad_times_input \u001b[38;5;241m=\u001b[39m input_grad \u001b[38;5;241m*\u001b[39m images\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Ensure that images require gradients\n",
    "        images.requires_grad_()\n",
    "        \n",
    "        # Keep labels as integers\n",
    "        labels = labels.long()\n",
    "        \n",
    "        outputs = base_model(images) # Pour VGGFace\n",
    "        # outputs = model(images)    # Pour VGG16\n",
    "\n",
    "        # Calculate the gradient of the output with respect to the input\n",
    "        grad_output = torch.ones(outputs.size(), requires_grad=True)  # Set requires_grad=True\n",
    "        input_grad = torch.autograd.grad(outputs, images, grad_outputs=grad_output, retain_graph=True)[0]\n",
    "\n",
    "        # Calculate the gradient times the input\n",
    "        grad_times_input = input_grad * images\n",
    "        # Define your attribution maps (a_l) and prior maps (a*) as torch Tensors\n",
    "        attribution_maps = grad_times_input  \n",
    "        # prior_maps = torch.tensor(images)   \n",
    "        prior_maps = images.clone().detach() #Recommandé par une erreur python\n",
    "\n",
    "        # Create an instance of the PrivilegedAttributionLoss\n",
    "        pal_loss_fn = PrivilegedAttributionLoss()\n",
    "\n",
    "        # Calculate the PAL loss\n",
    "        pal_loss = pal_loss_fn(attribution_maps, prior_maps)\n",
    "        classification_loss = 0\n",
    "        # Add the PAL loss to your total loss (cross-entropy or other)\n",
    "        total_loss = classification_loss + pal_loss\n",
    "\n",
    "        if epoch == 0 :\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "\n",
    "            # Assuming grad_times_input is a torch.Tensor and images is a tensor\n",
    "            grad_times_input_np = grad_times_input[0].cpu().detach().numpy()  # Convert to NumPy array\n",
    "            grad_times_input_rescaled = (grad_times_input_np - grad_times_input_np.min()) / (grad_times_input_np.max() - grad_times_input_np.min())  # Rescale to [0, 1]\n",
    "\n",
    "            # Convert the original tensor (images) to a NumPy array for visualization\n",
    "            original_image_np = images[0].permute(1, 2, 0).cpu().detach().numpy()\n",
    "\n",
    "            # Create a figure with two subplots\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "            # Display the original image on the first subplot\n",
    "            axes[0].imshow(original_image_np)\n",
    "            axes[0].axis('off')\n",
    "            axes[0].set_title('Original Image')\n",
    "\n",
    "            # Display the gradient input as an image on the second subplot\n",
    "            axes[1].imshow(grad_times_input_rescaled.transpose(1, 2, 0))  # Transpose dimensions if needed\n",
    "            axes[1].axis('off')\n",
    "            axes[1].set_title('Gradient Input')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d7785-0df7-408f-b32f-598ea24b7deb",
   "metadata": {},
   "source": [
    "# Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23529fb-8571-4bcd-a6ca-4bd5a2e3e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        # outputs = model(images) # Pour VGG16\n",
    "        outputs = base_model(images) # Pour VGGFace\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on the test set: {100 * correct / total}%')\n",
    "    \n",
    "print(\"résultat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bf49d-1286-41b5-bf8b-2488b2203e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
