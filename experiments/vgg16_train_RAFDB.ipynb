{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from datasets import load_dataset\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import face_recognition\n",
    "import os\n",
    "import random\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class RAFDBDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_dir, subset, label_file_name, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.subset = subset\n",
    "        self.label_file_name = label_file_name\n",
    "        self.labels, self.image_paths = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        labels = []\n",
    "        image_paths = []\n",
    "        \n",
    "        labels_file_path = os.path.join(self.label_dir, self.label_file_name)\n",
    "        with open(labels_file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(' ')\n",
    "                label = int(parts[1])\n",
    "                image_path = os.path.join(self.root_dir, self.subset, parts[0])\n",
    "                labels.append(label)\n",
    "                image_paths.append(image_path)\n",
    "\n",
    "        return labels, image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_generator(image):\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "    # Load the pre-trained facial landmark model\n",
    "    face_landmarks_list = face_recognition.face_landmarks(image, face_locations)\n",
    "\n",
    "    h,w = image.shape[:2]\n",
    "    lm = np.zeros([h,w])\n",
    "\n",
    "    # Draw facial landmarks on the image\n",
    "    for face_landmarks in face_landmarks_list:\n",
    "        for landmark_type, landmarks in face_landmarks.items():\n",
    "            for (x, y) in landmarks:\n",
    "                if x < h and y < w :\n",
    "                    lm[y,x] = 1\n",
    "\n",
    "    heatmap = cv2.GaussianBlur(lm, [59,59], 3)         \n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def generate_batch_heatmaps(images, heatmap_generator):\n",
    "    batch_heatmaps = torch.zeros_like(images)\n",
    "\n",
    "    for i in range(images.size(0)):\n",
    "        # Convertir le tenseur PyTorch en tableau NumPy pour l'image i\n",
    "        image_np = images[i].permute(1, 2, 0).cpu().detach().numpy()\n",
    "        image_np = (image_np * 255).astype(np.uint8) if image_np.dtype != np.uint8 else image_np\n",
    "        image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        # Générer la heatmap pour l'image actuelle\n",
    "        heatmap_np = heatmap_generator(image_np)\n",
    "        heatmap_tensor = torch.from_numpy(heatmap_np).float().unsqueeze(0)\n",
    "\n",
    "        # Normaliser la heatmap et l'adapter à la taille de l'image\n",
    "        heatmap_tensor = heatmap_tensor / torch.max(heatmap_tensor)\n",
    "        heatmap_tensor = heatmap_tensor.repeat(3, 1, 1)\n",
    "\n",
    "        # Stocker la heatmap dans le tenseur batch\n",
    "        batch_heatmaps[i] = heatmap_tensor\n",
    "    \n",
    "    return batch_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PrivilegedAttributionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrivilegedAttributionLoss, self).__init__()\n",
    "\n",
    "    def forward(self, attribution_maps, prior_maps):\n",
    "        # Add a small value to standard deviation to avoid division by zero\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        # Calculate mean and standard deviation for each sample in the batch\n",
    "        mean_al = torch.mean(attribution_maps, dim=[1, 2, 3], keepdim=True)  # Assuming BCHW format\n",
    "        std_al = torch.std(attribution_maps, dim=[1, 2, 3], keepdim=True) + epsilon\n",
    "\n",
    "        # Replace NaN values with a default value (e.g., 0) in attribution_maps, mean_al, std_al, and prior_maps\n",
    "        attribution_maps = torch.where(torch.isnan(attribution_maps), torch.zeros_like(attribution_maps), attribution_maps)\n",
    "        mean_al = torch.where(torch.isnan(mean_al), torch.zeros_like(mean_al), mean_al)\n",
    "        std_al = torch.where(torch.isnan(std_al), torch.zeros_like(std_al), std_al)\n",
    "        prior_maps = torch.where(torch.isnan(prior_maps), torch.zeros_like(prior_maps), prior_maps)\n",
    "\n",
    "        # Calculate the PAL loss\n",
    "        # Ensure that the broadcasting in the subtraction and division is correct\n",
    "        pal_loss = -torch.sum((attribution_maps - mean_al) / std_al * prior_maps, dim=[1, 2, 3])\n",
    "\n",
    "        # Return the mean loss over the batch\n",
    "        return torch.mean(pal_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAFDB DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "              VGG-39                 [-1, 4096]               0\n",
      "           Linear-40                    [-1, 8]          32,776\n",
      "================================================================\n",
      "Total params: 134,293,320\n",
      "Trainable params: 134,293,320\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.80\n",
      "Params size (MB): 512.29\n",
      "Estimated Total Size (MB): 731.67\n",
      "----------------------------------------------------------------\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "RAF-DB Dataset Loaded !\n"
     ]
    }
   ],
   "source": [
    "# Transform function for train data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=(-10, 10)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Transform function for test data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "root_dir = '../datasets/RAF-DB/Image/aligned/'\n",
    "label_dir = '../datasets/RAF-DB/Image/aligned/labels'\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have already defined full_dataset, train_subset, and test_subset\n",
    "train_dataset = RAFDBDataset(root_dir=root_dir, label_dir = label_dir, subset = 'train', label_file_name='train_label.txt', transform=train_transform)\n",
    "test_dataset = RAFDBDataset(root_dir=root_dir, label_dir = label_dir, subset = 'test', label_file_name='test_label.txt', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Charger le modèle pré-entraîné VGG16\n",
    "base_model = torchvision.models.vgg16(pretrained=True)\n",
    "# Supprimer la dernière couche entièrement connectée\n",
    "base_model.classifier = nn.Sequential(*list(base_model.classifier.children())[:-1])\n",
    "\n",
    "# Ajouter une nouvelle couche adaptée à 7 classes\n",
    "num_classes = 8\n",
    "classifier_layer = nn.Linear(4096, num_classes)\n",
    "model = nn.Sequential(base_model, classifier_layer)\n",
    "\n",
    "# Afficher la structure du modèle\n",
    "summary(model, (3, 224, 224))  # Assurez-vous d'ajuster les dimensions en fonction de vos données\n",
    "\n",
    "# Identifier la dernière couche de convolution\n",
    "last_conv_layer = model[0].features[28]\n",
    "print(last_conv_layer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Fonction pour enregistrer le gradient\n",
    "def save_gradient(grad):\n",
    "    global conv_output_gradient\n",
    "    conv_output_gradient = grad\n",
    "\n",
    "print(\"RAF-DB Dataset Loaded !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def plot_element(images, batch_heatmaps, attribution_maps, gradients, i):\n",
    "    plt.figure(figsize=(12, 8))  # Agrandir la figure pour accueillir toutes les visualisations\n",
    "    # Afficher l'image original\n",
    "    plt.subplot(2, 4, 1)\n",
    "    image_to_show = images[i].permute(1, 2, 0).cpu().detach().numpy()\n",
    "    image_to_show = (image_to_show - image_to_show.min()) / (image_to_show.max() - image_to_show.min())  # Normalisation\n",
    "    image_to_show = np.clip(image_to_show, 0, 1) \n",
    "    plt.imshow(image_to_show)\n",
    "    plt.title('Image')\n",
    "\n",
    "    # Afficher la heatmap\n",
    "    plt.subplot(2, 4, 2)\n",
    "    heatmap_to_show = batch_heatmaps[i].permute(1, 2, 0).cpu().detach().numpy()\n",
    "    heatmap_to_show = np.clip(heatmap_to_show, 0, 1) \n",
    "    plt.imshow(heatmap_to_show)\n",
    "    plt.title('Heatmap')\n",
    "\n",
    "    # Afficher chaque canal de la carte d'attribution\n",
    "    plt.subplot(2, 4, 3)\n",
    "    attribution_to_show = attribution_maps[i].detach().permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Normaliser les valeurs de la carte d'attribution dans la plage [0, 1]\n",
    "    scaler = MinMaxScaler()\n",
    "    attribution_norm = scaler.fit_transform(attribution_to_show.reshape(-1, 1)).reshape(attribution_to_show.shape)\n",
    "    \n",
    "    attribution_mean = np.mean(attribution_norm, axis=2)\n",
    "    \n",
    "    # Appliquer une colormap 'jet' pour obtenir une carte d'attribution colorée\n",
    "    cmap = plt.get_cmap('bwr')\n",
    "    attribution_colored = cmap(attribution_mean)\n",
    "\n",
    "    # Supprimer le canal alpha retourné par la colormap\n",
    "    attribution_colored = attribution_colored[..., :3]\n",
    "    overlayed_image = (image_to_show) * 0.2 + attribution_colored * 0.9  # Ajustez la transparence ici\n",
    "    overlayed_image = np.clip(overlayed_image, 0, 1) \n",
    "    plt.imshow(overlayed_image)\n",
    "    plt.title(\"Carte d'atribution\")\n",
    "\n",
    "    # Afficher le gradient de sortie sur l'image originale\n",
    "    plt.subplot(2, 4, 4)\n",
    "    gradients_to_show = gradients[i].detach().permute(1, 2, 0).cpu().numpy()\n",
    "    gradients_to_show = np.abs(gradients_to_show)\n",
    "    gradients_to_show /= np.max(gradients_to_show)\n",
    "                \n",
    "    # Superposer le gradient sur l'image originale\n",
    "    overlayed_image = (gradients_to_show * 1.5 + image_to_show * 0.2)\n",
    "    overlayed_image = np.clip(overlayed_image, 0, 1) \n",
    "    plt.imshow(overlayed_image)\n",
    "    plt.title('Gradient')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/767 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15/767 [02:10<1:48:45,  8.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Backpropagation et optimisation\u001b[39;00m\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients before the backward pass\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Mise à jour des running loss et PAL loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Attacher un hook pour enregistrer le gradient\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import torch\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss_values = [] \n",
    "accuracy_values = []  \n",
    "lr = 5e-5\n",
    "power = 5\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, num_epochs, initial_lr, power):\n",
    "    \"\"\"Ajuste le taux d'apprentissage selon une politique de décroissance polynomiale.\"\"\"\n",
    "    lr = initial_lr * (1 - (epoch / num_epochs)) ** power\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_learning_rate(optimizer, epoch, num_epochs, lr, power)  # Mise à jour du taux d'apprentissage\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_pal_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    total_samples = 0.0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        \n",
    "        # Initialiser un tenseur pour stocker toutes les heatmaps\n",
    "        batch_heatmaps = generate_batch_heatmaps(images, heatmap_generator)\n",
    "            \n",
    "         # Ensure that images require gradients\n",
    "        images.requires_grad_()\n",
    "\n",
    "         # Forward pass\n",
    "        outputs = model(images)\n",
    "        labels = labels.long()\n",
    "\n",
    "        # Calcul de la classification loss\n",
    "        classification_loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass for gradients with respect to the input images\n",
    "        classification_loss.backward(retain_graph=True)  \n",
    "        gradients = images.grad\n",
    "\n",
    "        # Compute the attribution maps as the element-wise product of the gradients and the input images\n",
    "        attribution_maps = gradients * images\n",
    "\n",
    "        # Compute the PAL loss using the attribution maps and the prior maps\n",
    "        pal_loss_fn = PrivilegedAttributionLoss()\n",
    "        pal_loss = pal_loss_fn(attribution_maps, batch_heatmaps)\n",
    "\n",
    "        # Calcul de la PAL loss et de la classification loss\n",
    "        total_loss = classification_loss + pal_loss\n",
    "\n",
    "        # Backpropagation et optimisation\n",
    "        optimizer.zero_grad()  # Clear gradients before the backward pass\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Mise à jour des running loss et PAL loss\n",
    "        running_loss += classification_loss.item()\n",
    "        running_pal_loss += pal_loss.item()         \n",
    "\n",
    "\n",
    "\n",
    "        # Mise à jour des running loss et PAL loss\n",
    "        running_loss += classification_loss.item()\n",
    "        running_pal_loss += pal_loss.item()\n",
    "\n",
    "        # Calcul de l'accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "\n",
    "    # Afficher les visualisations pour la première image de la dernière batch\n",
    "    # plot_element(images, batch_heatmaps, attribution_maps, gradients, 0)\n",
    "    # Calcul des moyennes pour l'époque\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_pal_loss = running_pal_loss / len(train_loader)\n",
    "    epoch_acc = running_corrects.double() / total_samples\n",
    "\n",
    "    # Ajouter les valeurs moyennes aux listes\n",
    "    loss_values.append(epoch_loss)\n",
    "    accuracy_values.append(epoch_acc)\n",
    "\n",
    "    # Affichage des résultats pour l'époque\n",
    "    print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "    print(f'Loss: {epoch_loss:.4f}, PAL Loss: {epoch_pal_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Liste de longueurs de vecteur que vous souhaitez utiliser\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m vector_lengths \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mloss_values\u001b[49m), \u001b[38;5;28mlen\u001b[39m(loss_values))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Plot de la perte en fonction de la longueur du vecteur\u001b[39;00m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_values' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Liste de longueurs de vecteur que vous souhaitez utiliser\n",
    "vector_lengths = np.linspace(0, len(loss_values), len(loss_values))\n",
    "\n",
    "# Plot de la perte en fonction de la longueur du vecteur\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(vector_lengths, loss_values, marker='o', linestyle='-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perte')\n",
    "plt.title(\"Loss en fonction de l'epoch\")\n",
    "\n",
    "# Plot de la précision en fonction de la longueur du vecteur\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(vector_lengths, accuracy_values, marker='o', linestyle='-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Accuracy en fonction de l'epoch\")\n",
    "\n",
    "plt.tight_layout()  # Pour éviter que les titres se chevauchent\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ros_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
