{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7593881-3ebd-4071-80ee-8648eedf5d23",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0a56c0a-2176-40ff-90e3-eb3afc31c531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "# import cv2  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3646e2-1daa-4b0d-b200-894fae0bc3e6",
   "metadata": {},
   "source": [
    "# AffectNet Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d59eaf-cc18-42ad-bda1-0cfd706d4192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded !\n"
     ]
    }
   ],
   "source": [
    "class AffectNetHqDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        # 'dataset' is now a subset of the original dataset\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = load_dataset(\"Piro17/affectnethq\", split='train')\n",
    "\n",
    "# Split the dataset into train and test subsets\n",
    "train_size = int(0.01 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_subset, test_subset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation((-10, 10)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create the dataset and dataloader using the subsets\n",
    "train_dataset = AffectNetHqDataset(Subset(full_dataset, train_subset.indices), transform=train_transform)\n",
    "test_dataset = AffectNetHqDataset(Subset(full_dataset, test_subset.indices), transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"Dataset Loaded !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a75012",
   "metadata": {},
   "source": [
    "# RAF-DB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAF-DB Dataset Loaded !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RAFDBDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_dir, subset, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.subset = subset\n",
    "        self.labels, self.image_paths = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        labels = []\n",
    "        image_paths = []\n",
    "\n",
    "        labels_file_path = os.path.join(self.label_dir, 'label.txt')\n",
    "        with open(labels_file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(' ')\n",
    "                label = int(parts[1])\n",
    "                image_path = os.path.join(self.root_dir, self.subset, parts[0])  # Change 'aligned' to the desired folder\n",
    "                labels.append(label)\n",
    "                image_paths.append(image_path)\n",
    "\n",
    "        return labels, image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Transform function for training data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=(-10, 10)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Transform function for test data\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "root_dir = '../datasets/RAF-DB/Image/aligned/'\n",
    "label_dir = '../datasets/RAF-DB/Image/aligned/'\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have already defined full_dataset, train_subset, and test_subset\n",
    "train_dataset = RAFDBDataset(root_dir=root_dir, label_dir = label_dir, subset = 'train')\n",
    "test_dataset = RAFDBDataset(root_dir=root_dir, label_dir = label_dir, subset = 'test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"RAF-DB Dataset Loaded !\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da01978-38c3-4d4d-b44f-bccf450b0107",
   "metadata": {},
   "source": [
    "# PAL Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a28e3848-bc40-453c-a933-7924c65cec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrivilegedAttributionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrivilegedAttributionLoss, self).__init__()\n",
    "\n",
    "    def forward(self, attribution_maps, prior_maps):\n",
    "        \"\"\"\n",
    "        Compute the Privileged Attribution Loss (PAL).\n",
    "\n",
    "        Args:\n",
    "            attribution_maps (torch.Tensor): Attribution maps (a_l) from your model.\n",
    "            prior_maps (torch.Tensor): Prior maps (a*) that highlight certain regions.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: PAL loss value.\n",
    "        \"\"\"\n",
    "        # Calculate mean and standard deviation of attribution maps (a_l)\n",
    "        mean_al = torch.mean(attribution_maps)\n",
    "        std_al = torch.std(attribution_maps)\n",
    "\n",
    "        # Calculate the PAL loss as described in the provided text\n",
    "        pal_loss = -torch.sum((attribution_maps - mean_al) / std_al * prior_maps)\n",
    "\n",
    "        return pal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cd2710-3fc1-424b-a31f-a26ad66e0411",
   "metadata": {},
   "source": [
    "# Modèle VGGFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718520e0-403d-4452-b039-1a54cecf2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGFace(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFace, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Premier bloc\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Deuxième bloc\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Troisième bloc\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Quatrième bloc\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Cinquième bloc\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),  # Ajout de BatchNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # self.last_conv_layer = self.conv_layers[-1]\n",
    "        # self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.fc_layers = nn.Sequential(\n",
    "        #     nn.Linear(512, 1024),  \n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Dropout(0.5),\n",
    "        #     nn.Linear(1024, 7),\n",
    "        #     nn.Softmax(dim=1)\n",
    "        # )\n",
    "\n",
    "        self.last_conv_layer = self.conv_layers[-1]\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512, 4096),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 7),  # Assuming there are 7 classes in the RAF-DB dataset\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Attribute to store the gradients of the last convolutional layer\n",
    "        self.last_conv_gradients = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    def capture_last_conv_gradients(self):\n",
    "        return self.last_conv_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef5ae4b-1dc5-479b-ac82-a23740fad6b9",
   "metadata": {},
   "source": [
    "# Création des modèles pour les boucles d'entraînements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02cc3c04-5c13-4cdc-90cd-644b0ac1af35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "              VGG-39                 [-1, 4096]               0\n",
      "           Linear-40                    [-1, 7]          28,679\n",
      "================================================================\n",
      "Total params: 134,289,223\n",
      "Trainable params: 134,289,223\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.80\n",
      "Params size (MB): 512.27\n",
      "Estimated Total Size (MB): 731.65\n",
      "----------------------------------------------------------------\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle pré-entraîné VGG16\n",
    "base_model = torchvision.models.vgg16(pretrained=True)\n",
    "# Supprimer la dernière couche entièrement connectée\n",
    "base_model.classifier = nn.Sequential(*list(base_model.classifier.children())[:-1])\n",
    "\n",
    "# Ajouter une nouvelle couche adaptée à 7 classes\n",
    "num_classes = 7\n",
    "classifier_layer = nn.Linear(4096, num_classes)\n",
    "model = nn.Sequential(base_model, classifier_layer)\n",
    "\n",
    "# Afficher la structure du modèle\n",
    "summary(model, (3, 224, 224))  # Assurez-vous d'ajuster les dimensions en fonction de vos données\n",
    "\n",
    "\n",
    "# Identifier la dernière couche de convolution\n",
    "\n",
    "last_conv_layer = model[0].features[28]\n",
    "print(last_conv_layer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=4e-5)\n",
    "\n",
    "# Fonction pour enregistrer le gradient\n",
    "def save_gradient(grad):\n",
    "    global conv_output_gradient\n",
    "    conv_output_gradient = grad\n",
    "\n",
    "# Attacher un hook pour enregistrer le gradient\n",
    "last_conv_layer.register_backward_hook(lambda module, grad_in, grad_out: save_gradient(grad_out[0]))\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd003fe-c3ee-4645-8e50-bded71fbfe03",
   "metadata": {},
   "source": [
    "# Boucle d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f23b2323-e299-4fd9-812d-34ad57321d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/959 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/RAF-DB/Image/aligned/train/test_2346_aligned.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Ensure that images require gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[idx]\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m image_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_paths[idx]\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(image_path)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/models/TestModels.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(image)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:3243\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3240\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3242\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3243\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3244\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3246\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/RAF-DB/Image/aligned/train/test_2346_aligned.jpg'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Ensure that images require gradients\n",
    "        images.requires_grad_()\n",
    "        \n",
    "        # Keep labels as integers\n",
    "        labels = labels.long()\n",
    "        \n",
    "        outputs = base_model(images) # Pour VGGFace\n",
    "        # outputs = model(images)    # Pour VGG16\n",
    "\n",
    "        # Calculate the gradient of the output with respect to the input\n",
    "        grad_output = torch.ones(outputs.size(), requires_grad=True)  # Set requires_grad=True\n",
    "        input_grad = torch.autograd.grad(outputs, images, grad_outputs=grad_output, retain_graph=True)[0]\n",
    "\n",
    "        # Calculate the gradient times the input\n",
    "        grad_times_input = input_grad * images\n",
    "        # Define your attribution maps (a_l) and prior maps (a*) as torch Tensors\n",
    "        attribution_maps = grad_times_input  \n",
    "        # prior_maps = torch.tensor(images)   \n",
    "        prior_maps = images.clone().detach() #Recommandé par une erreur python\n",
    "\n",
    "        # Create an instance of the PrivilegedAttributionLoss\n",
    "        pal_loss_fn = PrivilegedAttributionLoss()\n",
    "\n",
    "        # Calculate the PAL loss\n",
    "        pal_loss = pal_loss_fn(attribution_maps, prior_maps)\n",
    "        classification_loss = 0\n",
    "        # Add the PAL loss to your total loss (cross-entropy or other)\n",
    "        total_loss = classification_loss + pal_loss\n",
    "\n",
    "        if epoch == 0 :\n",
    "            import matplotlib.pyplot as plt\n",
    "            import numpy as np\n",
    "\n",
    "            # Assuming grad_times_input is a torch.Tensor and images is a tensor\n",
    "            grad_times_input_np = grad_times_input[0].cpu().detach().numpy()  # Convert to NumPy array\n",
    "            grad_times_input_rescaled = (grad_times_input_np - grad_times_input_np.min()) / (grad_times_input_np.max() - grad_times_input_np.min())  # Rescale to [0, 1]\n",
    "\n",
    "            # Convert the original tensor (images) to a NumPy array for visualization\n",
    "            original_image_np = images[0].permute(1, 2, 0).cpu().detach().numpy()\n",
    "\n",
    "            # Create a figure with two subplots\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "            # Display the original image on the first subplot\n",
    "            axes[0].imshow(original_image_np)\n",
    "            axes[0].axis('off')\n",
    "            axes[0].set_title('Original Image')\n",
    "\n",
    "            # Display the gradient input as an image on the second subplot\n",
    "            axes[1].imshow(grad_times_input_rescaled.transpose(1, 2, 0))  # Transpose dimensions if needed\n",
    "            axes[1].axis('off')\n",
    "            axes[1].set_title('Gradient Input')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d7785-0df7-408f-b32f-598ea24b7deb",
   "metadata": {},
   "source": [
    "# Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23529fb-8571-4bcd-a6ca-4bd5a2e3e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        # outputs = model(images) # Pour VGG16\n",
    "        outputs = base_model(images) # Pour VGGFace\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on the test set: {100 * correct / total}%')\n",
    "    \n",
    "print(\"résultat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bf49d-1286-41b5-bf8b-2488b2203e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
