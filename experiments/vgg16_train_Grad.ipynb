{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from datasets import load_dataset\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Face recognition library\n",
    "import face_recognition\n",
    "\n",
    "# Gradient library\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from pytorch_grad_cam import GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define useful classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AffectNet Dataset Class\n",
    "\n",
    "class AffectNetHqDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "# RAFDB Dataset Class\n",
    " \n",
    "class RAFDBDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_dir, subset, label_file_name, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.subset = subset\n",
    "        self.label_file_name = label_file_name\n",
    "        self.labels, self.image_paths = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        labels = []\n",
    "        image_paths = []\n",
    "        \n",
    "        labels_file_path = os.path.join(self.label_dir, self.label_file_name)\n",
    "        with open(labels_file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(' ')\n",
    "                label = int(parts[1])\n",
    "                image_path = os.path.join(self.root_dir, self.subset, parts[0])\n",
    "                labels.append(label)\n",
    "                image_paths.append(image_path)\n",
    "\n",
    "        return labels, image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps Generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_generator(image):\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "    # Load the pre-trained facial landmark model\n",
    "    face_landmarks_list = face_recognition.face_landmarks(image, face_locations)\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    lm = np.zeros([height,width])\n",
    "\n",
    "    # Draw facial landmarks on the image\n",
    "    for face_landmarks in face_landmarks_list:\n",
    "        for landmark_type, landmarks in face_landmarks.items():\n",
    "            for (x, y) in landmarks:\n",
    "                if x < height and y < width :\n",
    "                    lm[y,x] = 1\n",
    "\n",
    "    heatmap = cv2.GaussianBlur(lm, [59,59], 3)         \n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def generate_batch_heatmaps(images, heatmap_generator):\n",
    "    batch_heatmaps = torch.zeros_like(images)\n",
    "\n",
    "    for i in range(images.size(0)):\n",
    "        # Convertir le tenseur PyTorch en tableau NumPy pour l'image i\n",
    "        image_np = images[i].permute(1, 2, 0).cpu().detach().numpy()\n",
    "        image_np = (image_np * 255).astype(np.uint8) if image_np.dtype != np.uint8 else image_np\n",
    "        image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        # Générer la heatmap pour l'image actuelle\n",
    "        heatmap_np = heatmap_generator(image_np)\n",
    "        heatmap_tensor = torch.from_numpy(heatmap_np).float().unsqueeze(0)\n",
    "\n",
    "        # Normaliser la heatmap et l'adapter à la taille de l'image\n",
    "        heatmap_tensor = heatmap_tensor / torch.max(heatmap_tensor)\n",
    "        heatmap_tensor = heatmap_tensor.repeat(3, 1, 1)\n",
    "\n",
    "        # Stocker la heatmap dans le tenseur batch\n",
    "        batch_heatmaps[i] = heatmap_tensor\n",
    "    \n",
    "    return batch_heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privileged Attribution Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrivilegedAttributionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrivilegedAttributionLoss, self).__init__()\n",
    "\n",
    "    def forward(self, attribution_maps, prior_maps):\n",
    "        # Add a small value to standard deviation to avoid division by zero\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        # Calculate mean and standard deviation for each sample in the batch\n",
    "        mean_al = torch.mean(attribution_maps, dim=[1, 2, 3], keepdim=True)  # Assuming BCHW format\n",
    "        std_al = torch.std(attribution_maps, dim=[1, 2, 3], keepdim=True) + epsilon\n",
    "\n",
    "        # Calculate the PAL loss\n",
    "        # Ensure that the broadcasting in the subtraction and division is correct\n",
    "        pal_loss = -torch.sum((attribution_maps - mean_al) / std_al * prior_maps, dim=[1, 2, 3])\n",
    "\n",
    "        # Return the mean loss over the batch\n",
    "        return torch.mean(pal_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AffectNet Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffedecd7109243d3a8518ea966c59ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/16349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (C:/Users/lenov/.cache/huggingface/datasets/imagefolder/AffectNet-ac71cf06b4a145a7/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "full_dataset = load_dataset(\"../datasets/AffectNet\", split='train')\n",
    "\n",
    "# Split the dataset into train and test subsets\n",
    "train_size = int(0.2 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_subset, test_subset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation((-10, 10)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create the dataset and dataloader using the subsets\n",
    "train_dataset = AffectNetHqDataset(Subset(full_dataset, train_subset.indices), transform=train_transform)\n",
    "test_dataset = AffectNetHqDataset(Subset(full_dataset, test_subset.indices), transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAFDB Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform function for train data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=(-10, 10)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Transform function for test data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "root_dir = '../datasets/RAF-DB/Image/aligned/'\n",
    "label_dir = '../datasets/RAF-DB/Image/aligned/labels'\n",
    "\n",
    "# Create the dataset and dataloader using the subsets\n",
    "RAFDB_train_dataset = RAFDBDataset(root_dir=root_dir, label_dir = label_dir, subset = 'train', label_file_name='train_label.txt', transform=transform)\n",
    "RAFDB_test_dataset = RAFDBDataset(root_dir=root_dir, label_dir = label_dir, subset = 'test', label_file_name='test_label.txt', transform=transform)\n",
    "\n",
    "RAFDB_train_loader = DataLoader(RAFDB_train_dataset, batch_size=16, shuffle=True)\n",
    "RAFDB_test_loader = DataLoader(RAFDB_test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Ploting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_element(images, batch_heatmaps, attribution_maps, gradients, i):\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))  # Agrandir la figure pour accueillir toutes les visualisations\n",
    "    # Afficher l'image original\n",
    "    plt.subplot(2, 4, 1)\n",
    "    image_to_show = images[i].permute(1, 2, 0).cpu().detach().numpy()\n",
    "    plt.imshow(image_to_show)\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    # Afficher la heatmap\n",
    "    plt.subplot(2, 4, 2)\n",
    "    heatmap_to_show = batch_heatmaps[i].permute(1, 2, 0).cpu().detach().numpy()\n",
    "    plt.imshow(heatmap_to_show)\n",
    "    plt.title('Heatmap')\n",
    "\n",
    "    # Afficher chaque canal de la carte d'attribution\n",
    "    plt.subplot(2, 4, 3)\n",
    "    attribution_to_show = attribution_maps[i].detach().permute(1, 2, 0).cpu().numpy()\n",
    "    attribution_norm = (attribution_to_show - attribution_to_show.min()) / (attribution_to_show.max() - attribution_to_show.min())\n",
    "    attribution_mean = np.mean(attribution_norm, axis=2)\n",
    "    # Appliquer une colormap 'jet' pour obtenir une carte d'attribution colorée\n",
    "    cmap = plt.get_cmap('bwr')\n",
    "    attribution_colored = cmap(attribution_mean)\n",
    "\n",
    "    # Supprimer le canal alpha retourné par la colormap\n",
    "    attribution_colored = attribution_colored[..., :3]\n",
    "    overlayed_image = (image_to_show) * 0.2 + attribution_colored * 0.9  # Ajustez la transparence ici\n",
    "    plt.imshow(overlayed_image)\n",
    "    plt.title('Attribution Overlay on Original Image')\n",
    "\n",
    "    # Afficher le gradient de sortie sur l'image originale\n",
    "    plt.subplot(1, 4, 4)\n",
    "    gradients_to_show = gradients[i].detach().permute(1, 2, 0).cpu().numpy()\n",
    "    gradients_to_show = np.abs(gradients_to_show)\n",
    "    gradients_to_show /= np.max(gradients_to_show)\n",
    "                \n",
    "    # Superposer le gradient sur l'image originale\n",
    "    overlayed_image = (gradients_to_show * 1.5 + image_to_show * 0.2)\n",
    "    plt.imshow(overlayed_image)\n",
    "    plt.title('Gradient Overlay on Original Image')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenov\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lenov\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "              VGG-39                 [-1, 4096]               0\n",
      "           Linear-40                    [-1, 8]          32,776\n",
      "================================================================\n",
      "Total params: 134,293,320\n",
      "Trainable params: 134,293,320\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.80\n",
      "Params size (MB): 512.29\n",
      "Estimated Total Size (MB): 731.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle pré-entraîné VGG16\n",
    "base_model = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "# Supprimer la dernière couche entièrement connectée\n",
    "base_model.classifier = nn.Sequential(*list(base_model.classifier.children())[:-1])\n",
    "\n",
    "# Ajouter une nouvelle couche adaptée à 7 classes\n",
    "num_classes = 8\n",
    "classifier_layer = nn.Linear(4096, num_classes)\n",
    "model = nn.Sequential(base_model, classifier_layer)\n",
    "\n",
    "# Afficher la structure du modèle\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training VGG16 model using PAL loss based on gradient attribution method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train parameters\n",
    "num_epochs = 1\n",
    "optimizer = optim.Adam(model.parameters(), lr=4e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss_values = [] \n",
    "accuracy_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 96, 96]           1,792\n",
      "              ReLU-2           [-1, 64, 96, 96]               0\n",
      "            Conv2d-3           [-1, 64, 96, 96]          36,928\n",
      "              ReLU-4           [-1, 64, 96, 96]               0\n",
      "         MaxPool2d-5           [-1, 64, 48, 48]               0\n",
      "            Conv2d-6          [-1, 128, 48, 48]          73,856\n",
      "              ReLU-7          [-1, 128, 48, 48]               0\n",
      "            Conv2d-8          [-1, 128, 48, 48]         147,584\n",
      "              ReLU-9          [-1, 128, 48, 48]               0\n",
      "        MaxPool2d-10          [-1, 128, 24, 24]               0\n",
      "           Conv2d-11          [-1, 256, 24, 24]         295,168\n",
      "             ReLU-12          [-1, 256, 24, 24]               0\n",
      "           Conv2d-13          [-1, 256, 24, 24]         590,080\n",
      "             ReLU-14          [-1, 256, 24, 24]               0\n",
      "           Conv2d-15          [-1, 256, 24, 24]         590,080\n",
      "             ReLU-16          [-1, 256, 24, 24]               0\n",
      "        MaxPool2d-17          [-1, 256, 12, 12]               0\n",
      "           Conv2d-18          [-1, 512, 12, 12]       1,180,160\n",
      "             ReLU-19          [-1, 512, 12, 12]               0\n",
      "           Conv2d-20          [-1, 512, 12, 12]       2,359,808\n",
      "             ReLU-21          [-1, 512, 12, 12]               0\n",
      "           Conv2d-22          [-1, 512, 12, 12]       2,359,808\n",
      "             ReLU-23          [-1, 512, 12, 12]               0\n",
      "        MaxPool2d-24            [-1, 512, 6, 6]               0\n",
      "           Conv2d-25            [-1, 512, 6, 6]       2,359,808\n",
      "             ReLU-26            [-1, 512, 6, 6]               0\n",
      "           Conv2d-27            [-1, 512, 6, 6]       2,359,808\n",
      "             ReLU-28            [-1, 512, 6, 6]               0\n",
      "           Conv2d-29            [-1, 512, 6, 6]       2,359,808\n",
      "             ReLU-30            [-1, 512, 6, 6]               0\n",
      "        MaxPool2d-31            [-1, 512, 3, 3]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             Tanh-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 2048]       8,390,656\n",
      "             Tanh-37                 [-1, 2048]               0\n",
      "          Dropout-38                 [-1, 2048]               0\n",
      "           Linear-39                    [-1, 8]          16,392\n",
      "          Softmax-40                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 125,886,280\n",
      "Trainable params: 111,171,592\n",
      "Non-trainable params: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 40.45\n",
      "Params size (MB): 480.22\n",
      "Estimated Total Size (MB): 520.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Cassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        modules = list(vgg16.children())[:-1]\n",
    "        vgg16 = nn.Sequential(*modules)  # output (512, 7, 7)\n",
    "\n",
    "        for p in vgg16.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.feature_extractor = vgg16\n",
    "\n",
    "        self.fc1 = nn.Linear(7 * 7 * 512, 4096)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(2048, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x1 = self.feature_extractor(x)\n",
    "        x = x1.view(-1, 7 * 7 * 512)\n",
    "        x = self.tanh1(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.tanh2(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        out = self.softmax(x)\n",
    "\n",
    "        return out.float() , x1.float()\n",
    "\n",
    "model = Cassifier()\n",
    "summary(model, (3, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2c5a30784d45f9829e1be49a9e32dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lenov\\Desktop\\Cours Sorbonne\\MLA\\Projet\\9.Priviledge_Attribute_Loss_TEAM_5\\experiments\\vgg16_train_Grad.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenov/Desktop/Cours%20Sorbonne/MLA/Projet/9.Priviledge_Attribute_Loss_TEAM_5/experiments/vgg16_train_Grad.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Backpropagation et optimisation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenov/Desktop/Cours%20Sorbonne/MLA/Projet/9.Priviledge_Attribute_Loss_TEAM_5/experiments/vgg16_train_Grad.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Clear gradients before the backward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lenov/Desktop/Cours%20Sorbonne/MLA/Projet/9.Priviledge_Attribute_Loss_TEAM_5/experiments/vgg16_train_Grad.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m total_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenov/Desktop/Cours%20Sorbonne/MLA/Projet/9.Priviledge_Attribute_Loss_TEAM_5/experiments/vgg16_train_Grad.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenov/Desktop/Cours%20Sorbonne/MLA/Projet/9.Priviledge_Attribute_Loss_TEAM_5/experiments/vgg16_train_Grad.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# if epoch == 0:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenov/Desktop/Cours%20Sorbonne/MLA/Projet/9.Priviledge_Attribute_Loss_TEAM_5/experiments/vgg16_train_Grad.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m#     for i in range(images.size(0)):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenov/Desktop/Cours%20Sorbonne/MLA/Projet/9.Priviledge_Attribute_Loss_TEAM_5/experiments/vgg16_train_Grad.ipynb#X16sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m#         plot_element(images, batch_heatmaps, attribution_maps, gradients, i)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenov/Desktop/Cours%20Sorbonne/MLA/Projet/9.Priviledge_Attribute_Loss_TEAM_5/experiments/vgg16_train_Grad.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenov/Desktop/Cours%20Sorbonne/MLA/Projet/9.Priviledge_Attribute_Loss_TEAM_5/experiments/vgg16_train_Grad.ipynb#X16sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# Updating the running loss the PAL loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenov\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lenov\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_pal_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    total_samples = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        \n",
    "        # Initialiser un tenseur pour stocker toutes les heatmaps\n",
    "        batch_heatmaps = generate_batch_heatmaps(images, heatmap_generator)\n",
    "\n",
    "        # Ensure that images require gradients\n",
    "        images.requires_grad_()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, feature_map = model(images)\n",
    "        feature_map.retain_grad()\n",
    "        labels = labels.long()\n",
    "\n",
    "        # Compute the classification loss\n",
    "        classification_loss = criterion(outputs, labels)\n",
    "        classification_loss.backward(retain_graph=True) \n",
    "        \n",
    "        # Compute PAL loss\n",
    "        attribution_maps = torch.autograd.grad(feature_map.sum(), images, retain_graph=True)[0]\n",
    "\n",
    "        # Compute the PAL loss using the attribution maps and the prior maps\n",
    "        pal_loss_fn = PrivilegedAttributionLoss()\n",
    "        pal_loss = pal_loss_fn(attribution_maps, batch_heatmaps)\n",
    "\n",
    "        # Compute the total loss which the the sum of the classification loss and the PAL loss\n",
    "        total_loss = classification_loss + pal_loss\n",
    "\n",
    "        # Backpropagation et optimisation\n",
    "        optimizer.zero_grad()  # Clear gradients before the backward pass\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if epoch == 0:\n",
    "        #     for i in range(images.size(0)):\n",
    "        #         plot_element(images, batch_heatmaps, attribution_maps, gradients, i)\n",
    "\n",
    "        # Updating the running loss the PAL loss\n",
    "        running_loss += classification_loss.item()\n",
    "        running_pal_loss += pal_loss.item()\n",
    "\n",
    "        # Compute Accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # batch loss\n",
    "        print(f'classification_loss: {classification_loss[0]:.4f}, PAL Loss: {pal_loss_fn[0]:.4f}, Total loss: {total_loss[0]:.4f}')\n",
    "\n",
    "\n",
    "    # Calcul des moyennes pour l'époque\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_pal_loss = running_pal_loss / len(train_loader)\n",
    "    epoch_acc = running_corrects.double() / total_samples\n",
    "\n",
    "    # Ajouter les valeurs moyennes aux listes\n",
    "    loss_values.append(epoch_loss)\n",
    "    accuracy_values.append(epoch_acc)\n",
    "\n",
    "    # Print results for each epoch\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Loss: {epoch_loss:.4f}, PAL Loss: {epoch_pal_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1291e+00, -2.0103e+00, -2.1313e+00,  ..., -4.1434e+00,\n",
       "           -3.8891e+00, -8.6978e-01],\n",
       "          [ 1.6544e+00, -6.8111e+00, -2.9516e+00,  ..., -2.2777e+00,\n",
       "            3.9368e+00,  4.0415e+00],\n",
       "          [ 2.9856e+00, -1.3615e+00,  7.5917e-01,  ..., -4.6231e+00,\n",
       "           -3.7794e+00,  2.1555e+00],\n",
       "          ...,\n",
       "          [-6.6558e-01, -4.7520e+00, -8.9596e+00,  ...,  4.3536e-01,\n",
       "            2.3864e+00,  5.9227e-01],\n",
       "          [ 3.7438e+00,  1.3509e+00, -9.0750e-02,  ..., -1.3213e+00,\n",
       "            2.7212e-01,  1.5926e+00],\n",
       "          [ 6.9509e-01, -4.9025e-01,  2.6169e+00,  ..., -1.5424e+00,\n",
       "           -3.8647e-01,  3.8328e-01]],\n",
       "\n",
       "         [[-1.1352e+00, -4.5111e+00, -4.9007e+00,  ..., -6.3900e+00,\n",
       "           -8.4332e+00, -3.8369e+00],\n",
       "          [ 4.2360e+00, -8.9882e+00, -6.2475e+00,  ..., -2.6656e+00,\n",
       "           -1.1207e-02,  7.5836e-01],\n",
       "          [ 4.2663e+00, -3.0313e+00,  2.0828e+00,  ...,  6.6630e+00,\n",
       "           -7.2264e-01,  4.3403e+00],\n",
       "          ...,\n",
       "          [-1.2814e+00,  1.9406e-01,  6.4253e+00,  ..., -3.8930e+00,\n",
       "           -8.9500e-02, -2.5323e-01],\n",
       "          [ 6.8099e+00,  1.1942e+01,  1.8403e+01,  ..., -4.2836e+00,\n",
       "           -6.4843e-01,  2.0386e+00],\n",
       "          [ 2.5430e+00,  4.6183e+00,  9.4390e+00,  ..., -2.4442e+00,\n",
       "           -2.4485e-01,  9.0469e-01]],\n",
       "\n",
       "         [[-3.1433e-01, -4.2188e-01, -8.2738e-01,  ..., -3.5432e+00,\n",
       "           -3.9424e+00, -2.0411e+00],\n",
       "          [ 4.9282e+00,  1.3069e+00,  3.1278e+00,  ..., -5.7754e+00,\n",
       "           -6.2365e+00, -5.1275e+00],\n",
       "          [ 4.5496e+00,  3.8253e+00,  9.0621e+00,  ...,  7.8882e-01,\n",
       "           -5.4226e+00, -2.6579e-01],\n",
       "          ...,\n",
       "          [-4.1474e-01,  2.8202e+00,  7.3996e+00,  ..., -1.7931e+00,\n",
       "           -6.7110e-01, -7.0233e-01],\n",
       "          [ 2.2058e+00,  5.6157e+00,  9.6424e+00,  ..., -1.9992e-01,\n",
       "           -2.7521e-01,  9.5312e-01],\n",
       "          [ 1.1667e+00,  1.8245e+00,  4.2420e+00,  ..., -4.3866e-01,\n",
       "            3.6757e-01,  4.6503e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.1553e+00,  1.3759e+01,  7.7105e+00,  ...,  5.0098e-01,\n",
       "            5.9265e-01,  9.8516e-01],\n",
       "          [ 9.0083e+00,  2.8765e+01,  1.8272e+01,  ...,  1.5877e+00,\n",
       "            2.1667e+00,  1.0509e+00],\n",
       "          [ 9.7710e+00,  2.8362e+01,  1.6849e+01,  ...,  9.9988e-01,\n",
       "            2.7213e+00,  2.5288e+00],\n",
       "          ...,\n",
       "          [-1.8796e-01, -5.6968e-01, -9.1877e-01,  ..., -5.5613e-01,\n",
       "           -1.1775e+00, -2.0194e+00],\n",
       "          [ 1.4859e-01, -3.7814e-01, -7.4335e-01,  ..., -2.3309e+00,\n",
       "           -3.0081e-01,  2.1471e-01],\n",
       "          [ 4.8803e-02, -1.3372e-01, -1.3282e+00,  ..., -1.1534e+00,\n",
       "           -6.4169e-01,  5.4879e-01]],\n",
       "\n",
       "         [[ 9.5328e+00,  1.9263e+01,  1.2305e+01,  ...,  3.9607e+00,\n",
       "            3.5612e+00,  3.5735e+00],\n",
       "          [ 6.0190e+00,  1.1688e+01,  5.4340e+00,  ...,  6.1332e+00,\n",
       "            5.0749e+00,  2.7406e+00],\n",
       "          [ 2.0015e+00, -2.2934e-01, -6.1675e+00,  ...,  1.8894e+00,\n",
       "            1.4606e+00,  1.3345e+00],\n",
       "          ...,\n",
       "          [-3.8868e-02,  1.2715e+00,  1.7365e+00,  ...,  6.3975e-01,\n",
       "            1.3122e+00,  2.0478e+00],\n",
       "          [-2.6530e-01,  8.4588e-02,  5.7693e-01,  ..., -2.3820e+00,\n",
       "            8.2150e-01,  2.3156e+00],\n",
       "          [-2.7331e-01, -4.4029e-01, -1.6313e+00,  ..., -1.8579e+00,\n",
       "           -8.0474e-01,  1.6224e+00]],\n",
       "\n",
       "         [[ 3.8254e+00,  2.1523e+00,  1.5681e+00,  ...,  4.5795e-01,\n",
       "            3.8386e-01,  1.4967e+00],\n",
       "          [-4.5185e+00, -1.5347e+01, -9.5607e+00,  ..., -1.4917e-02,\n",
       "           -1.2933e+00, -3.9396e-01],\n",
       "          [-5.4594e+00, -1.9399e+01, -1.1318e+01,  ..., -1.9994e+00,\n",
       "           -3.7041e+00, -1.2315e+00],\n",
       "          ...,\n",
       "          [-1.7274e-01,  5.0089e-01,  1.2437e-01,  ..., -3.2129e+00,\n",
       "           -1.6485e+00, -1.3610e-01],\n",
       "          [-2.3006e-01, -1.7512e-01,  4.5270e-02,  ..., -3.1994e+00,\n",
       "           -1.2352e+00,  5.0213e-02],\n",
       "          [-1.1801e-01, -1.7017e-01, -8.3574e-01,  ..., -1.5702e+00,\n",
       "           -1.4841e+00,  3.4820e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.6274e-01, -3.8128e+00, -4.1530e+00,  ..., -1.1612e+00,\n",
       "           -1.6999e+00, -1.4124e+00],\n",
       "          [-1.2930e-01, -6.2553e+00, -7.1827e+00,  ..., -2.0400e+00,\n",
       "           -1.3468e+00, -2.6068e+00],\n",
       "          [-1.1712e+00, -5.7076e+00, -5.5698e+00,  ..., -1.1685e+00,\n",
       "           -2.2118e+00, -1.7561e+00],\n",
       "          ...,\n",
       "          [-2.8270e-02,  1.8786e+00, -5.9237e+00,  ...,  1.7995e-01,\n",
       "            1.9019e+00,  1.2624e+00],\n",
       "          [-1.4331e+00, -3.6918e+00, -1.5947e+00,  ...,  1.5265e+00,\n",
       "            3.7905e+00,  1.3556e+00],\n",
       "          [-1.7153e+00, -9.7431e+00, -2.0611e+00,  ...,  8.4047e-01,\n",
       "            1.6132e+00,  7.8506e-01]],\n",
       "\n",
       "         [[-1.3803e-01, -1.7214e+00, -8.9980e-01,  ..., -3.1018e+00,\n",
       "           -4.1252e+00, -1.6219e+00],\n",
       "          [ 1.4657e+00,  9.9369e-01,  3.6876e+00,  ...,  1.4533e+00,\n",
       "            6.9110e-01,  9.0843e-02],\n",
       "          [-7.4253e-01,  1.5727e+00,  6.8624e+00,  ...,  3.5260e+00,\n",
       "            1.5498e+00,  3.2490e+00],\n",
       "          ...,\n",
       "          [-4.1298e+00,  1.2028e+01,  1.0878e+01,  ...,  1.7882e-01,\n",
       "           -5.9844e-01, -1.0470e-01],\n",
       "          [-8.0898e+00,  1.1735e-01,  1.2900e+01,  ...,  1.1226e+00,\n",
       "            4.3385e-01, -1.5525e+00],\n",
       "          [-6.4179e+00, -1.0727e+01,  8.3276e+00,  ...,  9.1998e-01,\n",
       "           -3.5538e-01, -9.9037e-01]],\n",
       "\n",
       "         [[-4.4771e-01, -1.0536e+00, -8.9456e-01,  ..., -2.2600e+00,\n",
       "           -2.4471e+00, -1.0202e+00],\n",
       "          [-7.2363e-01, -1.3045e+00, -1.5493e+00,  ..., -1.0288e+00,\n",
       "           -2.1678e-01, -8.3605e-01],\n",
       "          [-1.9184e+00, -2.5274e+00, -1.3336e+00,  ..., -8.8516e-01,\n",
       "           -1.3114e-01,  1.1127e+00],\n",
       "          ...,\n",
       "          [-5.8164e+00,  3.3816e+00,  3.0568e+00,  ..., -6.7677e-01,\n",
       "           -1.1855e+00, -2.7285e-03],\n",
       "          [-5.0287e+00,  1.6075e+00,  8.1179e+00,  ..., -3.6282e-01,\n",
       "           -9.2809e-01, -1.2442e+00],\n",
       "          [-2.1889e+00, -1.7326e+00,  7.9737e+00,  ...,  3.6211e-01,\n",
       "           -5.1020e-01, -3.9901e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.6551e+00,  3.7732e+00,  3.1676e+00,  ...,  3.7772e-01,\n",
       "            6.9380e-01,  1.4340e+00],\n",
       "          [ 3.1519e+00,  6.7198e+00,  3.7019e+00,  ...,  1.6279e+00,\n",
       "            2.2925e+00,  2.4638e+00],\n",
       "          [ 3.2033e+00,  6.4275e+00,  2.9607e+00,  ...,  1.4497e+00,\n",
       "            2.7436e+00,  2.9955e+00],\n",
       "          ...,\n",
       "          [-1.2754e+00,  3.8685e-01,  5.0552e-02,  ...,  1.6556e-02,\n",
       "            1.9149e+00,  1.5064e+00],\n",
       "          [-1.4757e+00, -1.3221e+00,  1.8244e+00,  ...,  1.7531e-02,\n",
       "            3.2579e+00,  2.4361e+00],\n",
       "          [-1.2720e+00, -3.3059e+00,  5.8459e-01,  ..., -3.0217e-01,\n",
       "            1.4877e+00,  1.7235e+00]],\n",
       "\n",
       "         [[ 8.2193e+00,  1.7083e+01,  1.6927e+01,  ...,  3.0089e+00,\n",
       "            2.6741e+00,  2.6504e+00],\n",
       "          [ 1.2095e+01,  2.4565e+01,  2.3501e+01,  ...,  2.5523e+00,\n",
       "            2.5113e+00,  1.9393e+00],\n",
       "          [ 1.1212e+01,  2.1828e+01,  1.9465e+01,  ..., -9.0379e-01,\n",
       "           -7.7268e-01, -1.5197e-02],\n",
       "          ...,\n",
       "          [-1.5269e+00,  2.0058e+00,  1.1092e+00,  ..., -4.0839e-02,\n",
       "           -1.5647e+00, -3.1448e-01],\n",
       "          [-6.3777e-01,  1.7023e+00,  4.1511e+00,  ..., -4.8939e-01,\n",
       "           -1.2999e+00, -1.0055e+00],\n",
       "          [-4.8794e-01, -1.3613e+00,  3.0882e+00,  ...,  2.5094e-01,\n",
       "           -6.0949e-01,  1.8922e-01]],\n",
       "\n",
       "         [[ 4.1627e+00,  1.4152e+00,  8.1475e-02,  ...,  2.1300e+00,\n",
       "            1.8874e+00,  1.7669e+00],\n",
       "          [-2.5295e-02, -9.9651e+00, -1.4852e+01,  ...,  1.5371e+00,\n",
       "            1.5718e+00,  1.1796e+00],\n",
       "          [-9.4819e-01, -1.4910e+01, -2.0685e+01,  ..., -1.2369e+00,\n",
       "           -1.1672e+00, -3.6543e-01],\n",
       "          ...,\n",
       "          [-4.3850e-01,  9.4831e-01, -1.3325e+00,  ..., -1.2828e+00,\n",
       "           -1.5501e+00, -3.0728e-01],\n",
       "          [ 1.2292e+00,  2.6509e+00,  2.6639e+00,  ..., -1.2093e+00,\n",
       "           -1.7389e+00, -1.1541e+00],\n",
       "          [ 1.3454e+00,  1.9023e+00,  2.8204e+00,  ...,  2.6258e-01,\n",
       "           -4.8454e-01,  1.0658e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.0758e-01, -3.8820e-03,  6.6018e-01,  ...,  9.9366e-01,\n",
       "            1.8565e+00,  1.0992e+00],\n",
       "          [ 4.0629e-01,  1.6861e+00,  2.9438e+00,  ...,  2.1982e+00,\n",
       "            1.5230e+00,  2.1072e+00],\n",
       "          [-7.8617e-01,  2.7951e-01,  2.8666e+00,  ...,  1.5253e+00,\n",
       "            6.3848e-02,  6.9672e-01],\n",
       "          ...,\n",
       "          [ 7.3849e-01,  2.2543e+00,  5.3652e-01,  ..., -7.7286e-01,\n",
       "            2.5630e+00,  2.7632e+00],\n",
       "          [-5.2667e+00, -3.6920e+00,  1.9185e+00,  ...,  3.0996e-01,\n",
       "            6.7665e+00,  4.9283e+00],\n",
       "          [-1.9400e-01, -5.8415e+00,  6.7865e-01,  ...,  1.8116e+00,\n",
       "            4.0351e+00,  2.6844e+00]],\n",
       "\n",
       "         [[-7.8778e-01, -3.1564e+00, -2.3696e+00,  ...,  2.5780e+00,\n",
       "            4.2073e+00,  2.1087e+00],\n",
       "          [-1.3578e+00, -5.6608e+00, -4.5687e+00,  ...,  3.3634e+00,\n",
       "            3.4044e+00,  2.9305e+00],\n",
       "          [-2.8804e+00, -7.2248e+00, -4.5042e+00,  ...,  9.5854e-01,\n",
       "           -1.2317e+00, -8.6281e-01],\n",
       "          ...,\n",
       "          [-6.4175e-02, -5.5511e-01, -1.9210e+00,  ...,  4.4165e-01,\n",
       "           -2.8741e+00, -7.6238e-01],\n",
       "          [-7.0199e+00, -5.0249e+00,  2.9512e+00,  ...,  6.3862e-02,\n",
       "           -7.8039e-01, -1.1654e+00],\n",
       "          [-8.2222e-01, -7.8278e+00,  1.7910e+00,  ...,  2.8852e+00,\n",
       "           -1.1187e-01, -8.7922e-01]],\n",
       "\n",
       "         [[ 2.8901e-01, -2.1247e-01,  2.9832e-01,  ...,  1.1101e+00,\n",
       "            1.7652e+00,  6.9166e-01],\n",
       "          [ 7.3085e-01,  2.7010e-01,  8.6522e-01,  ...,  1.6107e+00,\n",
       "            9.8957e-01,  1.0216e+00],\n",
       "          [ 6.4658e-01,  3.2393e-01,  2.0776e+00,  ...,  3.6873e-01,\n",
       "           -9.2177e-01, -3.9912e-01],\n",
       "          ...,\n",
       "          [ 9.0097e-01,  6.4059e-01, -9.9042e-01,  ..., -2.3975e+00,\n",
       "           -4.7078e+00, -1.3683e+00],\n",
       "          [-6.8526e-01,  2.1496e+00,  2.9567e+00,  ..., -2.2875e+00,\n",
       "           -4.5093e+00, -2.4061e+00],\n",
       "          [ 1.4362e+00, -8.7854e-01,  2.1471e+00,  ...,  3.3958e-01,\n",
       "           -2.4065e+00, -1.0702e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.0504e+00,  1.2056e-01,  4.2086e-01,  ..., -1.7945e+00,\n",
       "           -1.0518e+00, -7.4588e-01],\n",
       "          [ 3.1178e+00,  7.6731e+00,  4.5790e+00,  ...,  1.5489e+00,\n",
       "            3.0471e+00,  1.5043e+00],\n",
       "          [ 3.0947e+00,  8.3129e+00,  6.2109e+00,  ...,  3.5357e+00,\n",
       "            5.6107e+00,  4.0819e+00],\n",
       "          ...,\n",
       "          [-3.2173e-01, -2.9722e+00, -2.3248e+00,  ...,  9.8921e-01,\n",
       "            5.5984e+00,  4.7251e+00],\n",
       "          [-2.5163e+00, -4.8134e+00, -7.2721e-01,  ...,  2.4859e+00,\n",
       "            9.7332e+00,  6.5853e+00],\n",
       "          [-4.1630e-01, -6.3944e+00, -1.9142e+00,  ...,  1.5376e+00,\n",
       "            5.3104e+00,  4.0629e+00]],\n",
       "\n",
       "         [[-3.1502e+00,  7.1094e-01,  9.0512e-01,  ..., -2.1198e+00,\n",
       "           -2.8453e+00, -2.7119e+00],\n",
       "          [ 3.3818e+00,  6.0965e+00,  2.0515e+00,  ...,  1.0634e-01,\n",
       "           -1.0135e+00, -3.2612e+00],\n",
       "          [ 1.4101e+00,  2.6340e+00,  1.2015e+00,  ...,  1.0403e+00,\n",
       "            8.9334e-01, -1.4270e-01],\n",
       "          ...,\n",
       "          [-1.1008e+00,  6.7109e-01,  7.9433e-01,  ...,  1.3566e+00,\n",
       "           -3.0040e+00, -1.5504e+00],\n",
       "          [-3.8959e+00, -1.6102e+00,  4.0041e+00,  ...,  6.5336e-01,\n",
       "           -2.0317e+00, -3.4252e+00],\n",
       "          [-6.4512e-01, -5.6433e+00,  9.4012e-01,  ...,  1.6243e+00,\n",
       "           -1.1653e+00, -1.5402e+00]],\n",
       "\n",
       "         [[-8.4296e-01, -1.1571e-01,  7.1491e-01,  ...,  5.3782e-01,\n",
       "            4.4264e-02, -1.9708e-01],\n",
       "          [ 1.7876e-01, -1.5144e+00, -9.8448e-01,  ...,  1.1798e-01,\n",
       "           -5.3643e-01, -1.5170e+00],\n",
       "          [-1.0106e+00, -3.6962e+00, -8.8008e-01,  ..., -1.9102e+00,\n",
       "           -2.0990e+00, -1.3186e+00],\n",
       "          ...,\n",
       "          [-5.0460e-01,  2.8796e+00,  8.2393e-01,  ..., -1.5990e+00,\n",
       "           -4.4037e+00, -1.4590e+00],\n",
       "          [-8.9737e-01,  3.4367e+00,  3.5733e+00,  ..., -2.1035e+00,\n",
       "           -5.2337e+00, -3.7157e+00],\n",
       "          [ 8.5899e-01,  3.4811e-01,  2.0009e+00,  ...,  4.5538e-01,\n",
       "           -2.6584e+00, -1.4175e+00]]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
