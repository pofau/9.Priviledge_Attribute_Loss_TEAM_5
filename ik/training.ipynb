{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import face_recognition\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape\n",
    "input_shape = (96, 96, 3)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAL_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, hms_dir, transform=None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.root_dir = hms_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted([f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))])\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "\n",
    "        for cls_idx, cls_name in enumerate(self.classes):\n",
    "            cls_dir = os.path.join(root_dir, cls_name)\n",
    "            hm_dir = os.path.join(hms_dir, cls_name)\n",
    "            for file_name in os.listdir(cls_dir):\n",
    "                image_path = os.path.join(cls_dir, file_name)\n",
    "                heatmap_path = os.path.join(hm_dir, str(file_name[:-4])+'.npy')\n",
    "                self.samples.append((image_path, cls_idx, heatmap_path))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        file_path, cls_idx, heatmap_path = self.samples[idx]\n",
    "\n",
    "        \n",
    "        image = Image.open(file_path)\n",
    "        heatmap = np.load(heatmap_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "\n",
    "        return image, cls_idx, heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "data_path = '../datasets/AffectNet'\n",
    "hm_path = '../datasets/HeatMaps'\n",
    "\n",
    "# Define the data generator\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Define the training data generator\n",
    "train_dataset = PAL_Dataset(\n",
    "    root_dir=data_path, \n",
    "    hms_dir=hm_path,\n",
    "    transform=data_transforms['train']\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        modules = list(vgg16.children())[:-1]\n",
    "        vgg16 = nn.Sequential(*modules)  # output (512, 7, 7)\n",
    "\n",
    "        for p in vgg16.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.feature_extractor = vgg16\n",
    "\n",
    "        self.fc1 = nn.Linear(7 * 7 * 512, 4096)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(2048, 8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x1 = self.feature_extractor(x)\n",
    "        x = x1.view(-1, 7 * 7 * 512)\n",
    "        x = self.tanh1(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.tanh2(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        out = self.softmax(x)\n",
    "\n",
    "        return out, x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 96, 96]           1,792\n",
      "              ReLU-2           [-1, 64, 96, 96]               0\n",
      "            Conv2d-3           [-1, 64, 96, 96]          36,928\n",
      "              ReLU-4           [-1, 64, 96, 96]               0\n",
      "         MaxPool2d-5           [-1, 64, 48, 48]               0\n",
      "            Conv2d-6          [-1, 128, 48, 48]          73,856\n",
      "              ReLU-7          [-1, 128, 48, 48]               0\n",
      "            Conv2d-8          [-1, 128, 48, 48]         147,584\n",
      "              ReLU-9          [-1, 128, 48, 48]               0\n",
      "        MaxPool2d-10          [-1, 128, 24, 24]               0\n",
      "           Conv2d-11          [-1, 256, 24, 24]         295,168\n",
      "             ReLU-12          [-1, 256, 24, 24]               0\n",
      "           Conv2d-13          [-1, 256, 24, 24]         590,080\n",
      "             ReLU-14          [-1, 256, 24, 24]               0\n",
      "           Conv2d-15          [-1, 256, 24, 24]         590,080\n",
      "             ReLU-16          [-1, 256, 24, 24]               0\n",
      "        MaxPool2d-17          [-1, 256, 12, 12]               0\n",
      "           Conv2d-18          [-1, 512, 12, 12]       1,180,160\n",
      "             ReLU-19          [-1, 512, 12, 12]               0\n",
      "           Conv2d-20          [-1, 512, 12, 12]       2,359,808\n",
      "             ReLU-21          [-1, 512, 12, 12]               0\n",
      "           Conv2d-22          [-1, 512, 12, 12]       2,359,808\n",
      "             ReLU-23          [-1, 512, 12, 12]               0\n",
      "        MaxPool2d-24            [-1, 512, 6, 6]               0\n",
      "           Conv2d-25            [-1, 512, 6, 6]       2,359,808\n",
      "             ReLU-26            [-1, 512, 6, 6]               0\n",
      "           Conv2d-27            [-1, 512, 6, 6]       2,359,808\n",
      "             ReLU-28            [-1, 512, 6, 6]               0\n",
      "           Conv2d-29            [-1, 512, 6, 6]       2,359,808\n",
      "             ReLU-30            [-1, 512, 6, 6]               0\n",
      "        MaxPool2d-31            [-1, 512, 3, 3]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             Tanh-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 2048]       8,390,656\n",
      "             Tanh-37                 [-1, 2048]               0\n",
      "          Dropout-38                 [-1, 2048]               0\n",
      "           Linear-39                    [-1, 8]          16,392\n",
      "          Softmax-40                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 125,886,280\n",
      "Trainable params: 111,171,592\n",
      "Non-trainable params: 14,714,688\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 40.45\n",
      "Params size (MB): 480.22\n",
      "Estimated Total Size (MB): 520.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Cassifier()\n",
    "summary(model, (3, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model with simpl loss\n",
    "# for epoch in range(epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in tqdm(enumerate(train_loader, 0), total=len(train_loader)):\n",
    "\n",
    "#         inputs, labels, _ = data\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs, _ = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 10 == 9:\n",
    "#             tqdm.write(f'loss: {running_loss / 10:.3f}', end='\\r')\n",
    "#             running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model with simpl loss\n",
    "# for epoch in range(epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in tqdm(enumerate(train_loader, 0), total=len(train_loader)):\n",
    "\n",
    "#         inputs, labels, heatmaps = data\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 10 == 9:\n",
    "#             tqdm.write(f'loss: {running_loss / 10:.3f}', end='\\r')\n",
    "#             running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ..\\torch\\csrc\\autograd\\engine.cpp:1176.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m loss \u001b[38;5;241m+\u001b[39m grad_loss\n\u001b[0;32m     19\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 21\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward(create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "inputs, labels, heatmaps = next(iter(train_loader))\n",
    "optimizer.zero_grad()\n",
    "inp = inputs[1].requires_grad_()\n",
    "\n",
    "output, last_conv = model(inp)\n",
    "\n",
    "C = int(512/2)\n",
    "mean_third_layer_output = torch.mean(last_conv[C:])\n",
    "gradient = torch.autograd.grad(mean_third_layer_output, inp, create_graph=True)[0]\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "grad_loss = mse_loss(gradient, heatmaps[1])\n",
    "\n",
    "loss = criterion(output[0], labels[1])\n",
    "\n",
    "\n",
    "alpha = 1\n",
    "total_loss = alpha * loss + grad_loss\n",
    "total_loss = total_loss.float()\n",
    "\n",
    "total_loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
