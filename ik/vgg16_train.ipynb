{
<<<<<<< Updated upstream
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [
            {
               "ename": "ImportError",
               "evalue": "libGL.so.1: cannot open shared object file: No such file or directory",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
                  "\u001b[1;32m/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
                  "\u001b[0;31mImportError\u001b[0m: libGL.so.1: cannot open shared object file: No such file or directory"
               ]
            }
         ],
         "source": [
            "# Import\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "import torch.optim as optim\n",
            "import torchvision\n",
            "import torchvision.models as models\n",
            "import torchvision.transforms as transforms\n",
            "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
            "from datasets import load_dataset\n",
            "from torchsummary import summary\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "from PIL import Image\n",
            "from tqdm import tqdm\n",
            "import time\n",
            "import cv2\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import os\n",
            "import random\n",
            "import glob\n",
            "from PIL import Image\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.optim as optim\n",
            "import torchvision\n",
            "import torchvision.transforms as transforms\n",
            "from torchvision import models\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "from torchsummary import summary\n",
            "from tqdm.auto import tqdm\n",
            "# import cv2\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import face_recognition\n",
            "import os\n",
            "import random\n",
            "from matplotlib.colors import LinearSegmentedColormap\n",
            "\n"
         ]
=======
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from datasets import load_dataset\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import face_recognition\n",
    "import os\n",
    "import random\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffectNetHqDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_generator(image):\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "    # Load the pre-trained facial landmark model\n",
    "    face_landmarks_list = face_recognition.face_landmarks(image, face_locations)\n",
    "\n",
    "    h,w = image.shape[:2]\n",
    "    lm = np.zeros([h,w])\n",
    "\n",
    "    # Draw facial landmarks on the image\n",
    "    for face_landmarks in face_landmarks_list:\n",
    "        for landmark_type, landmarks in face_landmarks.items():\n",
    "            for (x, y) in landmarks:\n",
    "                if x < h and y < w :\n",
    "                    lm[y,x] = 1\n",
    "\n",
    "    heatmap = cv2.GaussianBlur(lm, [59,59], 3)         \n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PrivilegedAttributionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrivilegedAttributionLoss, self).__init__()\n",
    "\n",
    "    def forward(self, attribution_maps, prior_maps):\n",
    "        # Add a small value to standard deviation to avoid division by zero\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        # Calculate mean and standard deviation for each sample in the batch\n",
    "        mean_al = torch.mean(attribution_maps, dim=[1, 2, 3], keepdim=True)  # Assuming BCHW format\n",
    "        std_al = torch.std(attribution_maps, dim=[1, 2, 3], keepdim=True) + epsilon\n",
    "\n",
    "        # Calculate the PAL loss\n",
    "        # Ensure that the broadcasting in the subtraction and division is correct\n",
    "        pal_loss = -torch.sum((attribution_maps - mean_al) / std_al * prior_maps, dim=[1, 2, 3])\n",
    "\n",
    "        # Return the mean loss over the batch\n",
    "        return torch.mean(pal_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to C:/Users/kobbi/.cache/huggingface/datasets/Piro17___parquet/Piro17--affectnethq-921e21af3d38cd75/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79885c250dca4802a36ba2b1738f1784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b03b85e5974bf4b6fa1ec512da9403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37745ba77d5f46e1911a60add825867b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/27823 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:1858\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1857\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 1858\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, table \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[0;32m   1859\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m writer\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m>\u001b[39m max_shard_size:\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\datasets\\packaged_modules\\parquet\\parquet.py:67\u001b[0m, in \u001b[0;36mParquet._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 67\u001b[0m     parquet_file \u001b[38;5;241m=\u001b[39m pq\u001b[38;5;241m.\u001b[39mParquetFile(f)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:323\u001b[0m, in \u001b[0;36mParquetFile.__init__\u001b[1;34m(self, source, metadata, common_metadata, read_dictionary, memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, filesystem)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader \u001b[38;5;241m=\u001b[39m ParquetReader()\n\u001b[1;32m--> 323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader\u001b[38;5;241m.\u001b[39mopen(\n\u001b[0;32m    324\u001b[0m     source, use_memory_map\u001b[38;5;241m=\u001b[39mmemory_map,\n\u001b[0;32m    325\u001b[0m     buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size, pre_buffer\u001b[38;5;241m=\u001b[39mpre_buffer,\n\u001b[0;32m    326\u001b[0m     read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary, metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    327\u001b[0m     coerce_int96_timestamp_unit\u001b[38;5;241m=\u001b[39mcoerce_int96_timestamp_unit,\n\u001b[0;32m    328\u001b[0m     decryption_properties\u001b[38;5;241m=\u001b[39mdecryption_properties,\n\u001b[0;32m    329\u001b[0m     thrift_string_size_limit\u001b[38;5;241m=\u001b[39mthrift_string_size_limit,\n\u001b[0;32m    330\u001b[0m     thrift_container_size_limit\u001b[38;5;241m=\u001b[39mthrift_container_size_limit,\n\u001b[0;32m    331\u001b[0m )\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommon_metadata \u001b[38;5;241m=\u001b[39m common_metadata\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\pyarrow\\_parquet.pyx:1227\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetReader.open\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the full dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPiro17/affectnethq\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\datasets\\load.py:1797\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1794\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 1797\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\n\u001b[0;32m   1798\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   1799\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   1800\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m   1801\u001b[0m     try_from_hf_gcs\u001b[38;5;241m=\u001b[39mtry_from_hf_gcs,\n\u001b[0;32m   1802\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[0;32m   1803\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   1804\u001b[0m )\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1808\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   1809\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:890\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    889\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[1;32m--> 890\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    891\u001b[0m         dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[0;32m    892\u001b[0m         verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m    893\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    894\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    895\u001b[0m     )\n\u001b[0;32m    896\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:985\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    981\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m--> 985\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split(split_generator, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs)\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m    992\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:1746\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[1;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[0;32m   1744\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[1;32m-> 1746\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[0;32m   1747\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[0;32m   1748\u001b[0m     ):\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   1750\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[1;32mc:\\Users\\kobbi\\anaconda3\\Lib\\site-packages\\datasets\\builder.py:1891\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, SchemaInferenceError) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1890\u001b[0m         e \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39m__context__\n\u001b[1;32m-> 1891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "from datasets import load_dataset\n",
    "full_dataset = load_dataset(\"Piro17/affectnethq\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/ros_env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/ros_env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "              VGG-39                 [-1, 4096]               0\n",
      "           Linear-40                    [-1, 7]          28,679\n",
      "================================================================\n",
      "Total params: 134,289,223\n",
      "Trainable params: 134,289,223\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.80\n",
      "Params size (MB): 512.27\n",
      "Estimated Total Size (MB): 731.65\n",
      "----------------------------------------------------------------\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the dataset into train and test subsets\n",
    "train_size = int(0.01 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_subset, test_subset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation((-10, 10)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader using the subsets\n",
    "train_dataset = AffectNetHqDataset(Subset(full_dataset, train_subset.indices), transform=train_transform)\n",
    "test_dataset = AffectNetHqDataset(Subset(full_dataset, test_subset.indices), transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Charger le modèle pré-entraîné VGG16\n",
    "base_model = torchvision.models.vgg16(pretrained=True)\n",
    "# Supprimer la dernière couche entièrement connectée\n",
    "base_model.classifier = nn.Sequential(*list(base_model.classifier.children())[:-1])\n",
    "\n",
    "# Ajouter une nouvelle couche adaptée à 7 classes\n",
    "num_classes = 7\n",
    "classifier_layer = nn.Linear(4096, num_classes)\n",
    "model = nn.Sequential(base_model, classifier_layer)\n",
    "\n",
    "# Afficher la structure du modèle\n",
    "summary(model, (3, 224, 224))  # Assurez-vous d'ajuster les dimensions en fonction de vos données\n",
    "\n",
    "# Identifier la dernière couche de convolution\n",
    "last_conv_layer = model[0].features[28]\n",
    "print(last_conv_layer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=4e-5)\n",
    "\n",
    "# Fonction pour enregistrer le gradient\n",
    "def save_gradient(grad):\n",
    "    global conv_output_gradient\n",
    "    conv_output_gradient = grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c1785e5d0243ac8caf0fd1854ec88a",
       "version_major": 2,
       "version_minor": 0
>>>>>>> Stashed changes
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Define the number of epochs\n",
            "epochs = 10\n",
            "\n",
            "# Define the batch size\n",
            "batch_size = 16\n",
            "\n",
            "# Define the input shape\n",
            "input_shape = (224, 224)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "class AffectNetHqDataset(Dataset):\n",
            "    def __init__(self, dataset, transform=None):\n",
            "        self.dataset = dataset\n",
            "        self.transform = transform\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.dataset)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        item = self.dataset[idx]\n",
            "        image = item['image']\n",
            "        label = item['label']\n",
            "\n",
            "        if self.transform:\n",
            "            image = self.transform(image)\n",
            "\n",
            "        return image, label"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "import torch\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "import torchvision.transforms as transforms\n",
            "from PIL import Image\n",
            "import numpy as np\n",
            "\n",
            "\n",
            "class RAFDBDataset(Dataset):\n",
            "    def __init__(self, root_dir, label_dir, subset, label_file_name, transform=None):\n",
            "        self.root_dir = root_dir\n",
            "        self.label_dir = label_dir\n",
            "        self.transform = transform\n",
            "        self.subset = subset\n",
            "        self.label_file_name = label_file_name\n",
            "        self.labels, self.image_paths = self._load_data()\n",
            "\n",
            "    def _load_data(self):\n",
            "        labels = []\n",
            "        image_paths = []\n",
            "        \n",
            "        labels_file_path = os.path.join(self.label_dir, self.label_file_name)\n",
            "        with open(labels_file_path, 'r') as file:\n",
            "            lines = file.readlines()\n",
            "\n",
            "            for line in lines:\n",
            "                parts = line.strip().split(' ')\n",
            "                label = int(parts[1])\n",
            "                image_path = os.path.join(self.root_dir, self.subset, parts[0])\n",
            "                labels.append(label)\n",
            "                image_paths.append(image_path)\n",
            "\n",
            "        return labels, image_paths\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.labels)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        label = self.labels[idx]\n",
            "        image_path = self.image_paths[idx]\n",
            "        image = Image.open(image_path).convert('RGB')\n",
            "    \n",
            "\n",
            "        if self.transform:\n",
            "            image = self.transform(image)\n",
            "\n",
            "        return image, label\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [],
         "source": [
            "def heatmap_generator(image):\n",
            "    face_locations = face_recognition.face_locations(image)\n",
            "\n",
            "    # Load the pre-trained facial landmark model\n",
            "    face_landmarks_list = face_recognition.face_landmarks(image, face_locations)\n",
            "\n",
            "    h,w = image.shape[:2]\n",
            "    lm = np.zeros([h,w])\n",
            "\n",
            "    # Draw facial landmarks on the image\n",
            "    for face_landmarks in face_landmarks_list:\n",
            "        for landmark_type, landmarks in face_landmarks.items():\n",
            "            for (x, y) in landmarks:\n",
            "                if x < h and y < w :\n",
            "                    lm[y,x] = 1\n",
            "\n",
            "    heatmap = cv2.GaussianBlur(lm, [59,59], 3)         \n",
            "\n",
            "    return heatmap"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "import torch.nn as nn\n",
            "\n",
            "class PrivilegedAttributionLoss(nn.Module):\n",
            "    def __init__(self):\n",
            "        super(PrivilegedAttributionLoss, self).__init__()\n",
            "\n",
            "    def forward(self, attribution_maps, prior_maps):\n",
            "        # Add a small value to standard deviation to avoid division by zero\n",
            "        epsilon = 1e-8\n",
            "\n",
            "        # Calculate mean and standard deviation for each sample in the batch\n",
            "        mean_al = torch.mean(attribution_maps, dim=[1, 2, 3], keepdim=True)  # Assuming BCHW format\n",
            "        std_al = torch.std(attribution_maps, dim=[1, 2, 3], keepdim=True) + epsilon\n",
            "\n",
            "        # Calculate the PAL loss\n",
            "        # Ensure that the broadcasting in the subtraction and division is correct\n",
            "        pal_loss = -torch.sum((attribution_maps - mean_al) / std_al * prior_maps, dim=[1, 2, 3])\n",
            "\n",
            "        # Return the mean loss over the batch\n",
            "        return torch.mean(pal_loss)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# RAF-DB Data Loader"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "----------------------------------------------------------------\n",
                  "        Layer (type)               Output Shape         Param #\n",
                  "================================================================\n",
                  "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
                  "              ReLU-2         [-1, 64, 224, 224]               0\n",
                  "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
                  "              ReLU-4         [-1, 64, 224, 224]               0\n",
                  "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
                  "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
                  "              ReLU-7        [-1, 128, 112, 112]               0\n",
                  "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
                  "              ReLU-9        [-1, 128, 112, 112]               0\n",
                  "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
                  "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
                  "             ReLU-12          [-1, 256, 56, 56]               0\n",
                  "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
                  "             ReLU-14          [-1, 256, 56, 56]               0\n",
                  "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
                  "             ReLU-16          [-1, 256, 56, 56]               0\n",
                  "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
                  "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
                  "             ReLU-19          [-1, 512, 28, 28]               0\n",
                  "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
                  "             ReLU-21          [-1, 512, 28, 28]               0\n",
                  "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
                  "             ReLU-23          [-1, 512, 28, 28]               0\n",
                  "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
                  "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
                  "             ReLU-26          [-1, 512, 14, 14]               0\n",
                  "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
                  "             ReLU-28          [-1, 512, 14, 14]               0\n",
                  "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
                  "             ReLU-30          [-1, 512, 14, 14]               0\n",
                  "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
                  "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
                  "           Linear-33                 [-1, 4096]     102,764,544\n",
                  "             ReLU-34                 [-1, 4096]               0\n",
                  "          Dropout-35                 [-1, 4096]               0\n",
                  "           Linear-36                 [-1, 4096]      16,781,312\n",
                  "             ReLU-37                 [-1, 4096]               0\n",
                  "          Dropout-38                 [-1, 4096]               0\n",
                  "              VGG-39                 [-1, 4096]               0\n",
                  "           Linear-40                    [-1, 7]          28,679\n",
                  "================================================================\n",
                  "Total params: 134,289,223\n",
                  "Trainable params: 134,289,223\n",
                  "Non-trainable params: 0\n",
                  "----------------------------------------------------------------\n",
                  "Input size (MB): 0.57\n",
                  "Forward/backward pass size (MB): 218.80\n",
                  "Params size (MB): 512.27\n",
                  "Estimated Total Size (MB): 731.65\n",
                  "----------------------------------------------------------------\n",
                  "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
                  "RAF-DB Dataset Loaded !\n"
               ]
            }
         ],
         "source": [
            "train_transform = transforms.Compose([\n",
            "    transforms.RandomRotation(degrees=(-10, 10)),\n",
            "    transforms.RandomHorizontalFlip(),\n",
            "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
            "    transforms.ToTensor(),\n",
            "])\n",
            "\n",
            "# Transform function for test data\n",
            "transform = transforms.Compose([\n",
            "\n",
            "    transforms.ToTensor(),\n",
            "])\n",
            "\n",
            "root_dir = '../datasets/RAF-DB/Image/aligned/'\n",
            "label_dir = '../datasets/RAF-DB/Image/aligned/labels'\n",
            "\n",
            "\n",
            "\n",
            "# Assuming you have already defined full_dataset, train_subset, and test_subset\n",
            "train_dataset = RAFDBDataset(root_dir=root_dir, label_dir = label_dir, subset = 'train', label_file_name='train_label.txt', transform=transform)\n",
            "test_dataset = RAFDBDataset(root_dir=root_dir, label_dir = label_dir, subset = 'test', label_file_name='test_label.txt', transform=transform)\n",
            "\n",
            "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
            "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
            "\n",
            "# Charger le modèle pré-entraîné VGG16\n",
            "base_model = torchvision.models.vgg16(pretrained=True)\n",
            "# Supprimer la dernière couche entièrement connectée\n",
            "base_model.classifier = nn.Sequential(*list(base_model.classifier.children())[:-1])\n",
            "\n",
            "# Ajouter une nouvelle couche adaptée à 7 classes\n",
            "num_classes = 7\n",
            "classifier_layer = nn.Linear(4096, num_classes)\n",
            "model = nn.Sequential(base_model, classifier_layer)\n",
            "\n",
            "# Afficher la structure du modèle\n",
            "summary(model, (3, 224, 224))  # Assurez-vous d'ajuster les dimensions en fonction de vos données\n",
            "\n",
            "# Identifier la dernière couche de convolution\n",
            "last_conv_layer = model[0].features[28]\n",
            "print(last_conv_layer)\n",
            "optimizer = optim.Adam(model.parameters(), lr=4e-5)\n",
            "\n",
            "# Fonction pour enregistrer le gradient\n",
            "def save_gradient(grad):\n",
            "    global conv_output_gradient\n",
            "    conv_output_gradient = grad\n",
            "\n",
            "print(\"RAF-DB Dataset Loaded !\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# AffectNet Data Loader"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Downloading readme: 100%|██████████| 574/574 [00:00<00:00, 2.94MB/s]\n",
                  "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
               ]
            },
            {
               "ename": "KeyboardInterrupt",
               "evalue": "",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                  "\u001b[1;32m/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the full dataset\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m full_dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mPiro17/affectnethq\u001b[39;49m\u001b[39m\"\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Split the dataset into train and test subsets\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m0.01\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(full_dataset))\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/load.py:2152\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2149\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2151\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2152\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   2153\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   2154\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   2155\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m   2156\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   2157\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   2158\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2159\u001b[0m )\n\u001b[1;32m   2161\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2162\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   2163\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   2164\u001b[0m )\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/builder.py:948\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 948\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    949\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    950\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m    951\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    952\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    953\u001b[0m     )\n\u001b[1;32m    954\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1021\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m split_dict \u001b[39m=\u001b[39m SplitDict(dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_name)\n\u001b[1;32m   1020\u001b[0m split_generators_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m-> 1021\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_split_generators(dl_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msplit_generators_kwargs)\n\u001b[1;32m   1023\u001b[0m \u001b[39m# Checksums verification\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[39mif\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mrecord_checksums:\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py:43\u001b[0m, in \u001b[0;36mParquet._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdata_files:\n\u001b[1;32m     42\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAt least one data file must be specified, but got data_files=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdata_files\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m data_files \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39;49mdownload_and_extract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdata_files)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_files, (\u001b[39mstr\u001b[39m, \u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m     45\u001b[0m     files \u001b[39m=\u001b[39m data_files\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/download/download_manager.py:561\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_and_extract\u001b[39m(\u001b[39mself\u001b[39m, url_or_urls):\n\u001b[1;32m    546\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[39m    Is roughly equivalent to:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload(url_or_urls))\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/download/download_manager.py:424\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    421\u001b[0m download_func \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download, download_config\u001b[39m=\u001b[39mdownload_config)\n\u001b[1;32m    423\u001b[0m start_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m--> 424\u001b[0m downloaded_path_or_paths \u001b[39m=\u001b[39m map_nested(\n\u001b[1;32m    425\u001b[0m     download_func,\n\u001b[1;32m    426\u001b[0m     url_or_urls,\n\u001b[1;32m    427\u001b[0m     map_tuple\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    428\u001b[0m     num_proc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mnum_proc,\n\u001b[1;32m    429\u001b[0m     disable_tqdm\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_progress_bar_enabled(),\n\u001b[1;32m    430\u001b[0m     desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDownloading data files\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    431\u001b[0m )\n\u001b[1;32m    432\u001b[0m duration \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    433\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading took \u001b[39m\u001b[39m{\u001b[39;00mduration\u001b[39m.\u001b[39mtotal_seconds()\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39m60\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m min\u001b[39m\u001b[39m\"\u001b[39m)\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/utils/py_utils.py:464\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    462\u001b[0m     num_proc \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(iterable) \u001b[39m<\u001b[39m parallel_min_length:\n\u001b[0;32m--> 464\u001b[0m     mapped \u001b[39m=\u001b[39m [\n\u001b[1;32m    465\u001b[0m         _single_map_nested((function, obj, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    466\u001b[0m         \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(iterable, disable\u001b[39m=\u001b[39mdisable_tqdm, desc\u001b[39m=\u001b[39mdesc)\n\u001b[1;32m    467\u001b[0m     ]\n\u001b[1;32m    468\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/utils/py_utils.py:465\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    462\u001b[0m     num_proc \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(iterable) \u001b[39m<\u001b[39m parallel_min_length:\n\u001b[1;32m    464\u001b[0m     mapped \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 465\u001b[0m         _single_map_nested((function, obj, types, \u001b[39mNone\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m))\n\u001b[1;32m    466\u001b[0m         \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(iterable, disable\u001b[39m=\u001b[39mdisable_tqdm, desc\u001b[39m=\u001b[39mdesc)\n\u001b[1;32m    467\u001b[0m     ]\n\u001b[1;32m    468\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/utils/py_utils.py:384\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: _single_map_nested((function, v, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m pbar}\n\u001b[1;32m    383\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     mapped \u001b[39m=\u001b[39m [_single_map_nested((function, v, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m)) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m pbar]\n\u001b[1;32m    385\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    386\u001b[0m         \u001b[39mreturn\u001b[39;00m mapped\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/utils/py_utils.py:384\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: _single_map_nested((function, v, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m pbar}\n\u001b[1;32m    383\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     mapped \u001b[39m=\u001b[39m [_single_map_nested((function, v, types, \u001b[39mNone\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m pbar]\n\u001b[1;32m    385\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    386\u001b[0m         \u001b[39mreturn\u001b[39;00m mapped\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/utils/py_utils.py:367\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39m# Singleton first to spare some computation\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, types):\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    369\u001b[0m \u001b[39m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39mif\u001b[39;00m rank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m logging\u001b[39m.\u001b[39mget_verbosity() \u001b[39m<\u001b[39m logging\u001b[39m.\u001b[39mWARNING:\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/download/download_manager.py:450\u001b[0m, in \u001b[0;36mDownloadManager._download\u001b[0;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[1;32m    448\u001b[0m     \u001b[39m# append the relative path to the base_path\u001b[39;00m\n\u001b[1;32m    449\u001b[0m     url_or_filename \u001b[39m=\u001b[39m url_or_path_join(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_path, url_or_filename)\n\u001b[0;32m--> 450\u001b[0m \u001b[39mreturn\u001b[39;00m cached_path(url_or_filename, download_config\u001b[39m=\u001b[39;49mdownload_config)\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/utils/file_utils.py:182\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     url_or_filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(url_or_filename)\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    181\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    183\u001b[0m         url_or_filename,\n\u001b[1;32m    184\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    185\u001b[0m         force_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_download,\n\u001b[1;32m    186\u001b[0m         proxies\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    187\u001b[0m         resume_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mresume_download,\n\u001b[1;32m    188\u001b[0m         user_agent\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muser_agent,\n\u001b[1;32m    189\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mlocal_files_only,\n\u001b[1;32m    190\u001b[0m         use_etag\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_etag,\n\u001b[1;32m    191\u001b[0m         max_retries\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    192\u001b[0m         token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mtoken,\n\u001b[1;32m    193\u001b[0m         ignore_url_params\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mignore_url_params,\n\u001b[1;32m    194\u001b[0m         storage_options\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[1;32m    195\u001b[0m         download_desc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mdownload_desc,\n\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    198\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/utils/file_utils.py:642\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, token, use_auth_token, ignore_url_params, storage_options, download_desc)\u001b[0m\n\u001b[1;32m    640\u001b[0m     ftp_get(url, temp_file)\n\u001b[1;32m    641\u001b[0m \u001b[39melif\u001b[39;00m scheme \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mhttp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhttps\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 642\u001b[0m     fsspec_get(url, temp_file, storage_options\u001b[39m=\u001b[39;49mstorage_options, desc\u001b[39m=\u001b[39;49mdownload_desc)\n\u001b[1;32m    643\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     http_get(\n\u001b[1;32m    645\u001b[0m         url,\n\u001b[1;32m    646\u001b[0m         temp_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    652\u001b[0m         desc\u001b[39m=\u001b[39mdownload_desc,\n\u001b[1;32m    653\u001b[0m     )\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/datasets/utils/file_utils.py:367\u001b[0m, in \u001b[0;36mfsspec_get\u001b[0;34m(url, temp_file, storage_options, desc)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGET can be called with at most one path but was called with \u001b[39m\u001b[39m{\u001b[39;00mpaths\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    359\u001b[0m callback \u001b[39m=\u001b[39m TqdmCallback(\n\u001b[1;32m    360\u001b[0m     tqdm_kwargs\u001b[39m=\u001b[39m{\n\u001b[1;32m    361\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdesc\u001b[39m\u001b[39m\"\u001b[39m: desc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mDownloading\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     }\n\u001b[1;32m    366\u001b[0m )\n\u001b[0;32m--> 367\u001b[0m fs\u001b[39m.\u001b[39;49mget_file(paths[\u001b[39m0\u001b[39;49m], temp_file\u001b[39m.\u001b[39;49mname, callback\u001b[39m=\u001b[39;49mcallback)\n",
                  "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fsspec/spec.py:914\u001b[0m, in \u001b[0;36mAbstractFileSystem.get_file\u001b[0;34m(self, rpath, lpath, callback, outfile, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39mwhile\u001b[39;00m data:\n\u001b[0;32m--> 914\u001b[0m     data \u001b[39m=\u001b[39m f1\u001b[39m.\u001b[39;49mread(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocksize)\n\u001b[1;32m    915\u001b[0m     segment_len \u001b[39m=\u001b[39m outfile\u001b[39m.\u001b[39mwrite(data)\n\u001b[1;32m    916\u001b[0m     \u001b[39mif\u001b[39;00m segment_len \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
                  "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fsspec/spec.py:1856\u001b[0m, in \u001b[0;36mAbstractBufferedFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[39mif\u001b[39;00m length \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1854\u001b[0m     \u001b[39m# don't even bother calling fetch\u001b[39;00m\n\u001b[1;32m   1855\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1856\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache\u001b[39m.\u001b[39;49m_fetch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc \u001b[39m+\u001b[39;49m length)\n\u001b[1;32m   1857\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(out)\n\u001b[1;32m   1858\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
                  "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fsspec/caching.py:189\u001b[0m, in \u001b[0;36mReadAheadCache._fetch\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    187\u001b[0m     part \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m end \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize, end \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocksize)\n\u001b[0;32m--> 189\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetcher(start, end)  \u001b[39m# new block replaces old\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart \u001b[39m=\u001b[39m start\n\u001b[1;32m    191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache)\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py:444\u001b[0m, in \u001b[0;36mHfFileSystemFile._fetch_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    433\u001b[0m headers \u001b[39m=\u001b[39m {\n\u001b[1;32m    434\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrange\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbytes=\u001b[39m\u001b[39m{\u001b[39;00mstart\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mend\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    435\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfs\u001b[39m.\u001b[39m_api\u001b[39m.\u001b[39m_build_hf_headers(),\n\u001b[1;32m    436\u001b[0m }\n\u001b[1;32m    437\u001b[0m url \u001b[39m=\u001b[39m hf_hub_url(\n\u001b[1;32m    438\u001b[0m     repo_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolved_path\u001b[39m.\u001b[39mrepo_id,\n\u001b[1;32m    439\u001b[0m     revision\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolved_path\u001b[39m.\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m     endpoint\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfs\u001b[39m.\u001b[39mendpoint,\n\u001b[1;32m    443\u001b[0m )\n\u001b[0;32m--> 444\u001b[0m r \u001b[39m=\u001b[39m http_backoff(\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    445\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mcontent\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:267\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    266\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m response \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    269\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
                  "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
                  "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:725\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    723\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 725\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m     history \u001b[39m=\u001b[39m []\n",
                  "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:725\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    723\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 725\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m     history \u001b[39m=\u001b[39m []\n",
                  "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:266\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    267\u001b[0m         req,\n\u001b[1;32m    268\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    269\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    270\u001b[0m         verify\u001b[39m=\u001b[39;49mverify,\n\u001b[1;32m    271\u001b[0m         cert\u001b[39m=\u001b[39;49mcert,\n\u001b[1;32m    272\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    273\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49madapter_kwargs,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m     extract_cookies_to_jar(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcookies, prepared_request, resp\u001b[39m.\u001b[39mraw)\n\u001b[1;32m    279\u001b[0m     \u001b[39m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
                  "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[0;32m--> 747\u001b[0m     r\u001b[39m.\u001b[39;49mcontent\n\u001b[1;32m    749\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
                  "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter_content(CONTENT_CHUNK_SIZE)) \u001b[39mor\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content_consumed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[39m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[39m# since we exhausted the data.\u001b[39;00m\n",
                  "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/urllib3/response.py:936\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 936\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    938\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    939\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    877\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 879\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    881\u001b[0m flush_decoder \u001b[39m=\u001b[39m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data)\n\u001b[1;32m    883\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/urllib3/response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    811\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    813\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 814\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    816\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/urllib3/response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    797\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
                  "File \u001b[0;32m~/.python/current/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
               ]
            }
         ],
         "source": [
            "# # Load the full dataset\n",
            "# full_dataset = load_dataset(\"Piro17/affectnethq\", split='train')\n",
            "\n",
            "# # Split the dataset into train and test subsets\n",
            "# train_size = int(0.01 * len(full_dataset))\n",
            "# test_size = len(full_dataset) - train_size\n",
            "# train_subset, test_subset = random_split(full_dataset, [train_size, test_size])\n",
            "\n",
            "# # Define transformations\n",
            "# train_transform = transforms.Compose([\n",
            "#     transforms.Resize((224, 224)),\n",
            "#     transforms.RandomRotation((-10, 10)),\n",
            "#     transforms.RandomHorizontalFlip(),\n",
            "#     transforms.ToTensor(),\n",
            "# ])\n",
            "\n",
            "# test_transform = transforms.Compose([\n",
            "#     transforms.Resize((224, 224)),\n",
            "#     transforms.ToTensor(),\n",
            "# ])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "# Create the dataset and dataloader using the subsets\n",
            "train_dataset = AffectNetHqDataset(Subset(full_dataset, train_subset.indices), transform=train_transform)\n",
            "test_dataset = AffectNetHqDataset(Subset(full_dataset, test_subset.indices), transform=test_transform)\n",
            "\n",
            "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
            "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
            "\n",
            "# Charger le modèle pré-entraîné VGG16\n",
            "base_model = torchvision.models.vgg16(pretrained=True)\n",
            "# Supprimer la dernière couche entièrement connectée\n",
            "base_model.classifier = nn.Sequential(*list(base_model.classifier.children())[:-1])\n",
            "\n",
            "# Ajouter une nouvelle couche adaptée à 7 classes\n",
            "num_classes = 7\n",
            "classifier_layer = nn.Linear(4096, num_classes)\n",
            "model = nn.Sequential(base_model, classifier_layer)\n",
            "\n",
            "# Afficher la structure du modèle\n",
            "summary(model, (3, 224, 224))  # Assurez-vous d'ajuster les dimensions en fonction de vos données\n",
            "\n",
            "# Identifier la dernière couche de convolution\n",
            "last_conv_layer = model[0].features[28]\n",
            "print(last_conv_layer)\n",
            "optimizer = optim.Adam(model.parameters(), lr=4e-5)\n",
            "\n",
            "# Fonction pour enregistrer le gradient\n",
            "def save_gradient(grad):\n",
            "    global conv_output_gradient\n",
            "    conv_output_gradient = grad"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Training Loop"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "\n",
                  "  0%|          | 0/767 [00:00<?, ?it/s]\n"
               ]
            },
            {
               "ename": "NameError",
               "evalue": "name 'cv2' is not defined",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                  "\u001b[1;32m/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     cv2_image \u001b[39m=\u001b[39m (cv2_image \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Convert from BGR to RGB for face recognition\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m cv2_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(cv2_image, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m heatmap \u001b[39m=\u001b[39m heatmap_generator(cv2_image)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-giggle-g9x5gw6gv56fvgqw/workspaces/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m heatmapa \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(heatmap)\u001b[39m.\u001b[39mfloat()\n",
                  "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
               ]
            }
         ],
         "source": [
            "# Attacher un hook pour enregistrer le gradient\n",
            "from matplotlib.colors import LinearSegmentedColormap\n",
            "\n",
            "# Identifier la dernière couche de convolution\n",
            "last_conv_layer = model[0].features[28]\n",
            "print(last_conv_layer)\n",
            "\n",
            "num_epochs = 1\n",
            "criterion = torch.nn.CrossEntropyLoss()\n",
            "\n",
            "for epoch in range(num_epochs):\n",
            "    model.train()\n",
            "    running_loss = 0.0\n",
            "    running_pal_loss = 0.0\n",
            "    running_corrects = 0.0\n",
            "    total_samples = 0.0\n",
            "    for images, labels in tqdm(train_loader):\n",
            "\n",
            "        # Convert the PyTorch tensor to a NumPy array\n",
            "        cv2_image = images[0].permute(1, 2, 0).cpu().detach().numpy()\n",
            "\n",
            "        # Ensure the image is in the correct type for OpenCV\n",
            "        if cv2_image.dtype != np.uint8:\n",
            "            cv2_image = (cv2_image * 255).astype(np.uint8)\n",
            "\n",
            "        # Convert from BGR to RGB for face recognition\n",
            "        cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
            "\n",
            "        heatmap = heatmap_generator(cv2_image)\n",
            "        heatmapa = torch.from_numpy(heatmap).float()\n",
            "\n",
            "        heatmapa = heatmapa / torch.max(heatmapa)  # Normalize the heatmap\n",
            "\n",
            "        # Ensure that images require gradients\n",
            "        images.requires_grad_()\n",
            "\n",
            "        # Forward pass\n",
            "        outputs = model(images)\n",
            "        labels = labels.long()\n",
            "\n",
            "        # Calcul de la classification loss\n",
            "        classification_loss = criterion(outputs, labels)\n",
            "\n",
            "        # Backward pass for gradients with respect to the input images\n",
            "        classification_loss.backward(retain_graph=True)  \n",
            "        gradients = images.grad ## Recuperate the gradients who saved in the hook\n",
            "\n",
            "        # Compute the attribution maps as the element-wise product of the gradients and the input images\n",
            "        attribution_maps = gradients * images\n",
            "\n",
            "        # Compute the PAL loss using the attribution maps and the prior maps\n",
            "        pal_loss_fn = PrivilegedAttributionLoss()\n",
            "        pal_loss = pal_loss_fn(attribution_maps, heatmapa)\n",
            "\n",
            "        # Calcul de la PAL loss et de la classification loss\n",
            "        total_loss = classification_loss + pal_loss\n",
            "\n",
            "        # Backpropagation et optimisation\n",
            "        optimizer.zero_grad()  # Clear gradients before the backward pass\n",
            "        total_loss.backward()\n",
            "        optimizer.step()\n",
            "\n",
            "        # Mise à jour des running loss et PAL loss\n",
            "        running_loss += classification_loss.item()\n",
            "        running_pal_loss += pal_loss.item()         \n",
            "\n",
            "        if epoch == 0 :\n",
            "            # Visualize the original image\n",
            "            plt.subplot(1, 3, 1)\n",
            "            plt.imshow(cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB))  \n",
            "            plt.title('Original Image')\n",
            "\n",
            "            # Visualize the heatmap\n",
            "            plt.subplot(1, 3, 2)\n",
            "            plt.imshow(heatmap, cmap='gray')\n",
            "            plt.title('Gradient Heatmap')\n",
            "\n",
            "            # Préparation de l'image du gradient\n",
            "            grad_times_input_np = gradients[0].cpu().detach().numpy()  # Récupérer les gradients pour la première image du lot\n",
            "            grad_times_input_rescaled = (grad_times_input_np - grad_times_input_np.min()) / (grad_times_input_np.max() - grad_times_input_np.min())  # Rescaler les gradients à [0, 1]\n",
            "            gradient_image = grad_times_input_rescaled.transpose(1, 2, 0)  # Transposer les dimensions si nécessaire\n",
            "            gradient_image_uint8 = np.uint8(255 * gradient_image)  # Convertir en entier 8 bits non signé\n",
            "\n",
            "            # Superposer l'image du gradient sur l'image d'origine\n",
            "            composite_image = cv2.addWeighted(cv2_image, 0.2, gradient_image_uint8, 1.5, 0)\n",
            "\n",
            "            # Afficher l'image composite\n",
            "            plt.subplot(1, 3, 3)\n",
            "            plt.imshow(composite_image)\n",
            "            plt.title('Image composite avec superposition du gradient')\n",
            "            plt.axis(\"off\")\n",
            "            plt.show()\n",
            "\n",
            "        # Calcul de la classification loss et de la PAL loss\n",
            "        total_loss = classification_loss + pal_loss\n",
            "\n",
            "        # Mise à jour des running loss et PAL loss\n",
            "        running_loss += classification_loss.item()\n",
            "        running_pal_loss += pal_loss.item()\n",
            "\n",
            "        # Calcul de l'accuracy\n",
            "        _, preds = torch.max(outputs, 1)\n",
            "        running_corrects += torch.sum(preds == labels.data)\n",
            "        total_samples += labels.size(0)\n",
            "\n",
            "     # Calcul des moyennes pour l'époque\n",
            "    epoch_loss = running_loss / len(train_loader)\n",
            "    epoch_pal_loss = running_pal_loss / len(train_loader)\n",
            "    epoch_acc = running_corrects.double() / total_samples\n",
            "\n",
            "    # Affichage des résultats\n",
            "    print(f'Epoch {epoch}/{num_epochs - 1}')\n",
            "    print(f'Loss: {epoch_loss:.4f}, PAL Loss: {epoch_pal_loss:.4f}, Accuracy: {epoch_acc:.4f}')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "ros_env",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.13"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}