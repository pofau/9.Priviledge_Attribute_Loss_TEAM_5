{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from datasets import load_dataset\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import face_recognition\n",
    "import os\n",
    "import random\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffectNetHqDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_generator(image):\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "    # Load the pre-trained facial landmark model\n",
    "    face_landmarks_list = face_recognition.face_landmarks(image, face_locations)\n",
    "\n",
    "    h,w = image.shape[:2]\n",
    "    lm = np.zeros([h,w])\n",
    "\n",
    "    # Draw facial landmarks on the image\n",
    "    for face_landmarks in face_landmarks_list:\n",
    "        for landmark_type, landmarks in face_landmarks.items():\n",
    "            for (x, y) in landmarks:\n",
    "                if x < h and y < w :\n",
    "                    lm[y,x] = 1\n",
    "\n",
    "    heatmap = cv2.GaussianBlur(lm, [59,59], 3)         \n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PrivilegedAttributionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrivilegedAttributionLoss, self).__init__()\n",
    "\n",
    "    def forward(self, attribution_maps, prior_maps):\n",
    "        # Add a small value to standard deviation to avoid division by zero\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        # Calculate mean and standard deviation for each sample in the batch\n",
    "        mean_al = torch.mean(attribution_maps, dim=[1, 2, 3], keepdim=True)  # Assuming BCHW format\n",
    "        std_al = torch.std(attribution_maps, dim=[1, 2, 3], keepdim=True) + epsilon\n",
    "\n",
    "        # Calculate the PAL loss\n",
    "        # Ensure that the broadcasting in the subtraction and division is correct\n",
    "        pal_loss = -torch.sum((attribution_maps - mean_al) / std_al * prior_maps, dim=[1, 2, 3])\n",
    "\n",
    "        # Return the mean loss over the batch\n",
    "        return torch.mean(pal_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/ros_env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/ros_env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "              VGG-39                 [-1, 4096]               0\n",
      "           Linear-40                    [-1, 7]          28,679\n",
      "================================================================\n",
      "Total params: 134,289,223\n",
      "Trainable params: 134,289,223\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.80\n",
      "Params size (MB): 512.27\n",
      "Estimated Total Size (MB): 731.65\n",
      "----------------------------------------------------------------\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "full_dataset = load_dataset(\"Piro17/affectnethq\", split='train')\n",
    "\n",
    "# Split the dataset into train and test subsets\n",
    "train_size = int(0.01 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_subset, test_subset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation((-10, 10)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create the dataset and dataloader using the subsets\n",
    "train_dataset = AffectNetHqDataset(Subset(full_dataset, train_subset.indices), transform=train_transform)\n",
    "test_dataset = AffectNetHqDataset(Subset(full_dataset, test_subset.indices), transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Charger le modèle pré-entraîné VGG16\n",
    "base_model = torchvision.models.vgg16(pretrained=True)\n",
    "# Supprimer la dernière couche entièrement connectée\n",
    "base_model.classifier = nn.Sequential(*list(base_model.classifier.children())[:-1])\n",
    "\n",
    "# Ajouter une nouvelle couche adaptée à 7 classes\n",
    "num_classes = 7\n",
    "classifier_layer = nn.Linear(4096, num_classes)\n",
    "model = nn.Sequential(base_model, classifier_layer)\n",
    "\n",
    "# Afficher la structure du modèle\n",
    "summary(model, (3, 224, 224))  # Assurez-vous d'ajuster les dimensions en fonction de vos données\n",
    "\n",
    "# Identifier la dernière couche de convolution\n",
    "last_conv_layer = model[0].features[28]\n",
    "print(last_conv_layer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=4e-5)\n",
    "\n",
    "# Fonction pour enregistrer le gradient\n",
    "def save_gradient(grad):\n",
    "    global conv_output_gradient\n",
    "    conv_output_gradient = grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e16d5417e743f68228709e34f7e3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/pofau/SAR/M2_SAR/M2_SAR/Machine Learning Avancé/Projet_MLA/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb Cellule 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pofau/SAR/M2_SAR/M2_SAR/Machine%20Learning%20Avanc%C3%A9/Projet_MLA/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m classification_loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pofau/SAR/M2_SAR/M2_SAR/Machine%20Learning%20Avanc%C3%A9/Projet_MLA/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# Compute the attribution maps as the element-wise product of the gradients and the input images\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/pofau/SAR/M2_SAR/M2_SAR/Machine%20Learning%20Avanc%C3%A9/Projet_MLA/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m attribution_maps \u001b[39m=\u001b[39m gradients \u001b[39m*\u001b[39;49m images\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pofau/SAR/M2_SAR/M2_SAR/Machine%20Learning%20Avanc%C3%A9/Projet_MLA/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Compute the PAL loss using the attribution maps and the prior maps\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pofau/SAR/M2_SAR/M2_SAR/Machine%20Learning%20Avanc%C3%A9/Projet_MLA/9.Priviledge_Attribute_Loss_TEAM_5/ik/vgg16_train.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m pal_loss_fn \u001b[39m=\u001b[39m PrivilegedAttributionLoss()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "# Attacher un hook pour enregistrer le gradient\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Identifier la dernière couche de convolution\n",
    "last_conv_layer = model[0].features[28]\n",
    "print(last_conv_layer)\n",
    "num_epochs = 1\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_pal_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    total_samples = 0.0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "\n",
    "        # Convert the PyTorch tensor to a NumPy array\n",
    "        cv2_image = images[0].permute(1, 2, 0).cpu().detach().numpy()\n",
    "\n",
    "        # Ensure the image is in the correct type for OpenCV\n",
    "        if cv2_image.dtype != np.uint8:\n",
    "            cv2_image = (cv2_image * 255).astype(np.uint8)\n",
    "\n",
    "        # Convert from BGR to RGB for face recognition\n",
    "        cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        heatmap = heatmap_generator(cv2_image)\n",
    "        heatmapa = torch.from_numpy(heatmap).float()\n",
    "\n",
    "        heatmapa = heatmapa / torch.max(heatmapa)  # Normalize the heatmap\n",
    "\n",
    "        # Ensure that images require gradients\n",
    "        images.requires_grad_()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        labels = labels.long()\n",
    "\n",
    "        # Calcul de la classification loss\n",
    "        classification_loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass for gradients with respect to the input images\n",
    "        classification_loss.backward(retain_graph=True)  \n",
    "        gradients = images.grad\n",
    "\n",
    "        # Compute the attribution maps as the element-wise product of the gradients and the input images\n",
    "        attribution_maps = gradients * images\n",
    "\n",
    "        # Compute the PAL loss using the attribution maps and the prior maps\n",
    "        pal_loss_fn = PrivilegedAttributionLoss()\n",
    "        pal_loss = pal_loss_fn(attribution_maps, heatmapa)\n",
    "\n",
    "        # Calcul de la PAL loss et de la classification loss\n",
    "        total_loss = classification_loss + pal_loss\n",
    "\n",
    "        # Backpropagation et optimisation\n",
    "        optimizer.zero_grad()  # Clear gradients before the backward pass\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Mise à jour des running loss et PAL loss\n",
    "        running_loss += classification_loss.item()\n",
    "        running_pal_loss += pal_loss.item()         \n",
    "\n",
    "        if epoch == 0 :\n",
    "            # Visualize the original image\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB))  # Convertir en RGB pour l'affichage\n",
    "            plt.title('Original Image')\n",
    "\n",
    "            # Visualize the heatmap\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(heatmap, cmap='gray')\n",
    "            plt.title('Gradient Heatmap')\n",
    "\n",
    "            # Préparation de l'image du gradient\n",
    "            grad_times_input_np = attribution_maps[0].cpu().detach().numpy()  # Use attribution_maps instead of pal_loss\n",
    "            grad_times_input_rescaled = (grad_times_input_np - grad_times_input_np.min()) / (grad_times_input_np.max() - grad_times_input_np.min())  # Rescale to [0, 1]\n",
    "            gradient_image = grad_times_input_rescaled.transpose(1, 2, 0)  # Transpose dimensions if needed\n",
    "            gradient_image_uint8 = np.uint8(255 * gradient_image)  # Convert to 8-bit unsigned integer\n",
    "\n",
    "            # Visualize the composite image\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(gradient_image_uint8)\n",
    "            plt.title('Composite Image with Gradient Overlay')\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        # Calcul de la classification loss et de la PAL loss\n",
    "        total_loss = classification_loss + pal_loss\n",
    "\n",
    "        # Mise à jour des running loss et PAL loss\n",
    "        running_loss += classification_loss.item()\n",
    "        running_pal_loss += pal_loss.item()\n",
    "\n",
    "        # Calcul de l'accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "     # Calcul des moyennes pour l'époque\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_pal_loss = running_pal_loss / len(train_loader)\n",
    "    epoch_acc = running_corrects.double() / total_samples\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "    print(f'Loss: {epoch_loss:.4f}, PAL Loss: {epoch_pal_loss:.4f}, Accuracy: {epoch_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ros_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
